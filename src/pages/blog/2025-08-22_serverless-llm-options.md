---
layout: ../../layouts/BlogLayout.astro
title: "3 Formas de Ejecutar Modelos LLM Open Source"
description: "Gu√≠a pr√°ctica: pruebas locales con Ollama, GUI con LM Studio y despliegues serverless escalables con vLLM en infraestructura on-demand."
tags: ["LLM", "AI", "Serverless", "vLLM", "Ollama", "LM Studio", "MLOps", "Infraestructura"]
time: 12 min read
timestamp: 2025-08-22T10:00:00+00:00
featured: false
filename: serverless-llm-options
---

# 3 Formas de Ejecutar Modelos LLM Open Source: de Local a Serverless

*Ollama ¬∑ LM Studio ¬∑ vLLM (serverless con Runpod / infraestructura on‚Äëdemand)*

La adopci√≥n de modelos open source (Qwen, Llama 3, DeepSeek, Mistral, etc.) est√° explotando. Ya no dependes exclusivamente de APIs cerradas: puedes iterar localmente, empaquetar prototipos y finalmente escalar a producci√≥n **sin bloquearte a un proveedor**.

En este post te cuento **tres enfoques complementarios** para ejecutar LLMs seg√∫n tu etapa: experimentaci√≥n, prototipado avanzado y producci√≥n serverless.

## üéØ Resumen R√°pido

| Caso de uso | Herramienta | Ventajas Clave | Trade-offs |
|-------------|-------------|----------------|------------|
| Aprender / Probar | Ollama | Instalaci√≥n m√≠nima, modelos cuantizados, CLI simple | Menos m√©tricas y tooling visual |
| Prototipar / Demo GUI | LM Studio | Interfaz gr√°fica, monitor de tokens/s, servidor HTTP, selecci√≥n HF | Requiere m√°s RAM/VRAM para fluidez |
| Producci√≥n escalable | vLLM + infraestructura serverless (Runpod, Modal, etc.) | Throughput alto, batching, paged attention, escalado horizontal | Curva de configuraci√≥n, costo si mal dimensionado |

---
## 1. üñ•Ô∏è Ollama: La Puerta de Entrada Simplicidad ‚ûú Iteraci√≥n Inmediata

**Cu√°ndo usarlo:** primeras pruebas, evaluar distintos modelos, fine-tuning ligero (con extensiones), construir scripts locales.

**Ventajas:**
- Instalaci√≥n en segundos (Mac/Linux/Windows WSL)
- Descarga y gesti√≥n de modelos unificada (`ollama pull`)
- Soporte para cuantizaciones que bajan el consumo de RAM/VRAM
- Sirve un endpoint local (`/api/generate`) para integraciones r√°pidas

**Ejemplo b√°sico:**
```bash
# Descargar y ejecutar (streaming en consola)
ollama run qwen2.5:7b

# Promptea directamente
> Eres un experto en Go. Explica qu√© es un goroutine en 2 frases.

# Usar como API local
curl http://localhost:11434/api/generate \
  -d '{"model": "qwen2.5:7b", "prompt": "Resume qu√© es paged attention"}'
```

**Regla mental de memoria (aprox.):**
- 7B cuantizado (Q4 / Q5 / MXFP4): 4‚Äì6 GB RAM
- 14B: 8‚Äì10 GB
- 70B: impr√°ctico local sin GPU grande

> ‚ö†Ô∏è Si tu m√°quina empieza a swappear, baja de tama√±o o usa una cuantizaci√≥n m√°s agresiva.

---
## 2. üé® LM Studio: Observabilidad y Control Visual

**Cu√°ndo usarlo:** demos a stakeholders, afinaci√≥n de prompts, medir tokens/s, probar m√∫ltiples modelos y configuraciones de sampling.

**Highlights:**
- UI amigable para gestionar descargas desde Hugging Face
- M√©tricas en tiempo real: velocidad (tokens/s), utilizaci√≥n
- Panel para par√°metros: temperature, top_p, repeat penalty
- Puede levantar un **servidor local** compatible con OpenAI API
- Exporta y reutiliza prompts / sesiones

**Ejecutar un servidor OpenAI-like:**
```bash
# En la UI activas: Enable Local Server
# Luego puedes consumirlo desde tu app:
export OPENAI_API_BASE=http://localhost:1234/v1
export OPENAI_API_KEY=sk-local

curl $OPENAI_API_BASE/chat/completions \
 -H "Authorization: Bearer $OPENAI_API_KEY" \
 -H "Content-Type: application/json" \
 -d '{
   "model": "qwen2.5:7b-instruct",
   "messages": [{"role": "user", "content": "Dame 3 ventajas de Go"}],
   "temperature": 0.7
 }'
```

**Rendimiento observado (ejemplo orientativo):**
- MacBook M‚Äëseries con suficiente RAM puede lograr **40‚Äì60 tokens/s** con modelos 7B cuantizados MXFP4.

**Cu√°ndo migrar m√°s all√° de LM Studio:** cuando necesitas **> concurrent users**, batching, throughput estable o reducir coste por token en producci√≥n.

---
## 3. ‚ö° vLLM + Infra Serverless: Camino a Producci√≥n

**Objetivo:** servir modelos con **bajo coste amortizado**, alta concurrencia y latencias predecibles.

**¬øPor qu√© vLLM?**
- Motor de inferencia optimizado (PagedAttention) ‚ûú mejor utilizaci√≥n de VRAM
- Batching din√°mico y speculative decoding (si activado)
- Manejo eficiente de long context windows
- Soporte OpenAI-compatible endpoints (`/v1/completions`, `/v1/chat/completions`)

**Arquitectura serverless t√≠pica (Runpod / Modal / Lambda GPU-like providers):**
```
Cliente ‚Üí API Gateway ‚Üí (Queue) ‚Üí Worker GPU (vLLM Engine) ‚Üí Respuesta
                     ‚Üò Autoscaler ‚Üó
```

**Componentes Clave:**
- Queue / dispatcher (async) para nivelar bursts
- M√∫ltiples r√©plicas vLLM detr√°s de un load balancer
- M√©tricas: tokens/s, time-to-first-token, GPU utilization, rejection rate
- Autoscaling por cola pendiente + GPU busy ratio

**Ejemplo de arranque vLLM (CLI):**
```bash
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --tensor-parallel-size 1 \
  --max-model-len 32768 \
  --port 8000 \
  --dtype auto \
  --enforce-eager
```

**Consumir el endpoint:**
```bash
curl http://localhost:8000/v1/chat/completions \
 -H "Content-Type: application/json" \
 -d '{
  "model": "Qwen2.5-7B-Instruct",
  "messages": [{"role": "user", "content": "Resume las ventajas de paged attention"}],
  "temperature": 0.3
 }'
```

### üì¶ Ejemplo Docker (simplificado)
```Dockerfile
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04
RUN apt-get update && apt-get install -y python3-pip git && rm -rf /var/lib/apt/lists/*
RUN pip install --no-cache-dir vllm==0.5.3
EXPOSE 8000
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--model", "Qwen/Qwen2.5-7B-Instruct", "--port", "8000"]
```

### üîÅ Autoscaling (idea conceptual)
- M√©trica primaria: `pending_requests` + `avg_time_to_first_token`
- Regla: si `pending_requests > X` durante Y segundos ‚ûú scale out
- Regla: si `gpu_utilization < 25%` y `pending_requests == 0` por Z segundos ‚ûú scale in

---
## üîß Cuantizaci√≥n (MXFP4 y otras)

**Por qu√© importa:** reduce VRAM/RAM y coste operativo manteniendo calidad razonable.

| Formato | Uso Memoria | Calidad | Caso T√≠pico |
|---------|-------------|---------|-------------|
| FP16 | Alto | M√°xima | Fine-tuning / evaluaci√≥n
| BF16 | Similar FP16 | Estable | GPUs modernas
| INT8 | Medio | Buena | Prototipos
| Q4_K / Q5_K | Bajo | Aceptable | Uso local
| **MXFP4** | Muy bajo | Sorprendentemente alta | Producci√≥n coste-eficiente

> Ajusta `max_model_len` y `max_num_seqs` para evitar OOM en contextos largos.

### üß¨ ¬øQu√© es MXFP4?

**MXFP4 (Mixed FP4)** es una familia de esquemas de 4 bits en formato flotante mixto (no estrictamente entero) que aprovecha mini‚Äëformatos FP4 (variantes como E2M1 / E3M0) y heur√≠sticas de escalado por bloque para mantener **m√°s rango din√°mico** que INT4 puro, reduciendo la p√©rdida de calidad t√≠pica de quantizaciones agresivas.

Caracter√≠sticas clave:
* Mezcla ("mixed") de sub‚Äëformatos para adaptar rango vs precisi√≥n seg√∫n la distribuci√≥n interna de pesos.
* Normalizaci√≥n / escala por bloque (8, 16, 32, 64 par√°metros) ‚Üí reduce outliers.
* Puede combinarse con dequant on‚Äëthe‚Äëfly + kernels fusionados para minimizar overhead.
* En inference: los pesos permanecen comprimidos hasta la etapa de multiplicaci√≥n (streaming decomp).

Comparaci√≥n (aprox.) memoria pesos (sin KV cache) para un modelo 7B:
| Representaci√≥n | Memoria ‚âà |
|----------------|-----------|
| FP16 | ~13.5‚Äì14 GB |
| INT8 (per‚Äëtensor) | ~7.0 GB |
| INT8 (per‚Äëchannel) | ~7.5 GB (ligero overhead metadatos) |
| Q4_K (LLM.int4) | ~3.8‚Äì4.2 GB |
| **MXFP4** | ~3.5‚Äì3.9 GB |

> Las cifras var√≠an por: embedding sharing, vocab size, formato KV, capas MoE, etc.

### üß™ M√©tricas de Calidad

Una forma com√∫n de evaluar la degradaci√≥n es la **p√©rdida de perplexity** (ŒîPPL) vs FP16 y la ca√≠da en benchmarks (MMLU, GSM8K...). MXFP4 bien calibrado suele mostrar:
* ŒîPPL t√≠pica: +0.5 a +1.2 sobre FP16 (menor que algunos INT4 uniformes)
* Ca√≠da en tareas razonamiento: 0‚Äì2 puntos porcentuales (depende del modelo)

### üß† GPT‚ÄëOSS con MXFP4

Cuando veas variantes tipo `gpt-oss-*` o `*-mxfp4` normalmente indica:
* **Pesos base** convertidos a formato MXFP4 post‚Äëtraining (PTQ) usando calibraci√≥n con subconjunto de datos.
* Bloques con escalado adaptativo para minimizar saturaci√≥n en capas sensibles (attention / layer norm adjacente).
* A veces mezcla: embedding en INT8 / capas cr√≠ticas en FP8 / resto FP4 ("mixed hybrid").

Beneficios pr√°cticos:
* Despliegas un modelo 7B en una GPU de 8‚Äì10GB manteniendo contexto ampliado (32K) sin OOM.
* Mayor throughput (menos tr√°fico de memoria) ‚Üí m√°s tokens/seg en vLLM.
* Coste por token inferior en entornos serverless.

Trade‚Äëoffs:
* Fine‚Äëtuning adicional (LoRA) encima de MXFP4 puede degradar si no re‚Äëcuantizas capas tocadas.
* Leve incremento de error de rounding en respuestas largas (>4K tokens) ‚Üí m√°s "drift".
* M√°s sensibilidad a prompts con muchas cifras / formatos estructurados (JSON estricto).

### üß© Estrategias de Quantizaci√≥n Complementarias

| Estrategia | Qu√© Cuantiza | Beneficio | Riesgo |
|------------|--------------|-----------|--------|
| Pesos (W) | Matrices de pesos | Ahorro VRAM principal | P√©rdida base si agresivo |
| Activaciones (A) | Output intermedio | Reduce picos VRAM | Puede introducir ruido acumulado |
| KV Cache | Claves/Valores atenci√≥n | Escala long context barato | Latencia extra por (de)quant |
| Mixta (W + KV) | Combinado | M√°ximo ahorro | Complejidad en kernels |

### üßÆ F√≥rmula Aproximada Memoria (Inference)

`Mem_Total ‚âà Mem_Pesos + (Batch * 2 * Layer * Head * Dim_Head * Bits_KV/8)`

Al usar MXFP4 s√≥lo en pesos (y mantener KV en FP16 / FP8), el crecimiento lineal del KV sigue dominando en contextos largos. Puedes aplicar KV INT8 / FP8 para extender contexto sin duplicar coste.

### ‚öôÔ∏è Ejemplos de Uso

vLLM (cuando soporte el flag espec√≠fico, hoy se hace v√≠a pesos ya cuantizados):
```bash
python -m vllm.entrypoints.openai.api_server \
  --model your-org/gpt-oss-7b-mxfp4 \
  --max-model-len 32768 \
  --tensor-parallel-size 1 \
  --enforce-eager
```

Transformers + bitsandbytes (4bit baseline) como alternativa si no hay MXFP4 nativo:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "your-org/gpt-oss-7b-mxfp4"  # si expone weights ya empaquetados
tok = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    load_in_4bit=True,          # fallback INT4 si no hay kernel MXFP4
    bnb_4bit_compute_dtype="bfloat16",
    bnb_4bit_quant_type="nf4"   # nf4 se aproxima cuando MXFP4 no disponible
)
```

KV Cache cuantizado (ejemplo conceptual en vLLM / configuraciones emergentes):
```bash
--quantization-config '{"kv_cache_dtype": "fp8", "weight_format": "mxfp4"}'
```

### üõë Pitfalls Espec√≠ficos MXFP4
* Reconvertir a FP16 para mezclar LoRA + MXFP4 puede invalidar la compresi√≥n (cadenas de conversi√≥n ‚Üí p√©rdida).
* Benchmarks cortos (short prompts) pueden ocultar degradaci√≥n en razonamiento largo.
* M√©tricas de calidad deben revisarse por dominio (code vs general chat).

### ‚úÖ Buenas Pr√°cticas
* Valida con un set de prompts cr√≠ticos (JSON, c√≥digo, reasoning, multi‚Äëturn).
* Ajusta `max_model_len` antes de escalar horizontalmente ‚Äî reduce costo inmediato.
* Monitorea `tokens/s` y `time_to_first_token` tras introducir quantizaci√≥n; deber√≠an mejorar o al menos no empeorar significativamente.
* Documenta el formato exacto (MXFP4 variante) para reproducibilidad.

---

---
## üß™ Benchmarks Mentales (Orientativos)

| Modelo (7B) | Entorno | Quant | Tokens/s (‚âà) | Notas |
|-------------|---------|-------|--------------|-------|
| Qwen 2.5 | Ollama local | Q4_K | 25‚Äì35 | Laptop M-series |
| Qwen 2.5 | LM Studio | MXFP4 | 40‚Äì60 | Mac M4 Pro 48GB |
| Qwen 2.5 | vLLM H100 | MXFP4 | 180‚Äì250 | Batching 8 req |

*No son cifras oficiales; sirven para ordenar magnitudes.*

---
## üß≠ C√≥mo Elegir

1. ¬øEst√°s aprendiendo? ‚ûú **Ollama**
2. ¬øNecesitas mostrar algo visual / iterar prompts con m√©tricas? ‚ûú **LM Studio**
3. ¬øTienes usuarios concurrentes / SLA / costo por token controlado? ‚ûú **vLLM serverless**

### Checklist Producci√≥n (vLLM)
- [ ] Limitar `max_model_len` (ej: 32K vs 256K si no lo necesitas)
- [ ] Activar logging estructurado
- [ ] M√©tricas: TTFB, tokens/s, cola, errores
- [ ] Health checks para readiness / liveness
- [ ] Estrategia de rotaci√≥n de versiones de modelo
- [ ] Pol√≠tica de escalado (fr√≠o/caliente)

---
## üí∞ Cost & Optimizaci√≥n

| Palanca | Impacto | Acci√≥n |
|---------|---------|--------|
| Cuantizaci√≥n | ‚Üì VRAM | MXFP4 / INT8 | 
| Contexto | ‚Üì Memoria & Latencia | Limita a 32K si 256K no aporta |
| Batching | ‚Üë Throughput | Ajusta tama√±o din√°mico |
| Streaming | ‚Üì Percepci√≥n de latencia | Env√≠a tokens tempranos |
| Warm Pools | ‚Üì Cold start | Mant√©n 1‚ÄìN r√©plicas calientes |

---
## ‚ö†Ô∏è Pitfalls Comunes
- Subir un contexto gigantesco para prompts simples
- No medir `time_to_first_token` (percepci√≥n usuario)
- Hacer scale-out sin l√≠mite ‚ûú costo explosivo nocturno
- Usar modelos demasiado grandes para casos que caben en 7B
- Ignorar l√≠mites de cuota del proveedor serverless

---
## üîÆ El Futuro Cercano
- M√°s **cuantizaciones h√≠bridas** manteniendo calidad casi FP16
- **Inference fusion** (operadores combinados) por defecto
- **Multi-tenancy** nativa m√°s segura en motores de inferencia
- Modelos 7B cada vez m√°s competitivos ‚Üí drenan necesidad de 70B
- Edge + on-device acelerado (NPUs / GPUs integradas)

---
## üõ†Ô∏è Snippet Comparativo (Mismo Prompt)

```bash
# Ollama
ollama run qwen2.5:7b "Explica en una frase qu√© es paged attention"

# LM Studio (OpenAI compatible)
curl $OPENAI_API_BASE/chat/completions \
 -H "Authorization: Bearer $OPENAI_API_KEY" \
 -H "Content-Type: application/json" \
 -d '{"model":"qwen2.5-7b","messages":[{"role":"user","content":"Explica en una frase qu√© es paged attention"}]}'

# vLLM
curl http://localhost:8000/v1/chat/completions \
 -H "Content-Type: application/json" \
 -d '{"model":"Qwen2.5-7B-Instruct","messages":[{"role":"user","content":"Explica en una frase qu√© es paged attention"}],"stream":true}'
```

---
## ‚úÖ Conclusi√≥n
No existe una √∫nica "mejor" forma de ejecutar LLMs. **Existe la correcta para tu fase actual.** Empieza simple, gana comprensi√≥n, a√±ade observabilidad y termina escalando con un motor preparado para producci√≥n.

¬øYa probaste alguna? ¬øPlaneas migrar a vLLM? Cu√©ntamelo y podemos extender esta gu√≠a con benchmarks reales.

---

#Ô∏è‚É£ **#LLM #AI #Serverless #vLLM #Ollama #LMStudio #MLOps #Infraestructura #OpenSource**
