/* empty css                                                                    */
import { d as createComponent, i as renderComponent, r as renderTemplate, u as unescapeHTML } from './astro/server_C7nAViGe.mjs';
import 'kleur/colors';
import { $ as $$BlogLayout } from './BlogLayout_COI89YL8.mjs';

const html = () => "<h1 id=\"3-formas-de-ejecutar-modelos-llm-open-source-de-local-a-serverless\">3 Formas de Ejecutar Modelos LLM Open Source: de Local a Serverless</h1>\n<p><em>Ollama ¬∑ LM Studio ¬∑ vLLM (serverless con Runpod / infraestructura on‚Äëdemand)</em></p>\n<p>La adopci√≥n de modelos open source (Qwen, Llama 3, DeepSeek, Mistral, etc.) est√° explotando. Ya no dependes exclusivamente de APIs cerradas: puedes iterar localmente, empaquetar prototipos y finalmente escalar a producci√≥n <strong>sin bloquearte a un proveedor</strong>.</p>\n<p>En este post te cuento <strong>tres enfoques complementarios</strong> para ejecutar LLMs seg√∫n tu etapa: experimentaci√≥n, prototipado avanzado y producci√≥n serverless.</p>\n<h2 id=\"-resumen-r√°pido\">üéØ Resumen R√°pido</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Caso de uso</th><th>Herramienta</th><th>Ventajas Clave</th><th>Trade-offs</th></tr></thead><tbody><tr><td>Aprender / Probar</td><td>Ollama</td><td>Instalaci√≥n m√≠nima, modelos cuantizados, CLI simple</td><td>Menos m√©tricas y tooling visual</td></tr><tr><td>Prototipar / Demo GUI</td><td>LM Studio</td><td>Interfaz gr√°fica, monitor de tokens/s, servidor HTTP, selecci√≥n HF</td><td>Requiere m√°s RAM/VRAM para fluidez</td></tr><tr><td>Producci√≥n escalable</td><td>vLLM + infraestructura serverless (Runpod, Modal, etc.)</td><td>Throughput alto, batching, paged attention, escalado horizontal</td><td>Curva de configuraci√≥n, costo si mal dimensionado</td></tr></tbody></table>\n<hr>\n<h2 id=\"1-Ô∏è-ollama-la-puerta-de-entrada-simplicidad--iteraci√≥n-inmediata\">1. üñ•Ô∏è Ollama: La Puerta de Entrada Simplicidad ‚ûú Iteraci√≥n Inmediata</h2>\n<p><strong>Cu√°ndo usarlo:</strong> primeras pruebas, evaluar distintos modelos, fine-tuning ligero (con extensiones), construir scripts locales.</p>\n<p><strong>Ventajas:</strong></p>\n<ul>\n<li>Instalaci√≥n en segundos (Mac/Linux/Windows WSL)</li>\n<li>Descarga y gesti√≥n de modelos unificada (<code>ollama pull</code>)</li>\n<li>Soporte para cuantizaciones que bajan el consumo de RAM/VRAM</li>\n<li>Sirve un endpoint local (<code>/api/generate</code>) para integraciones r√°pidas</li>\n</ul>\n<p><strong>Ejemplo b√°sico:</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Descargar y ejecutar (streaming en consola)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">ollama</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> qwen2.5:7b</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Promptea directamente</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> Eres un experto en Go. Explica qu√© es un goroutine en 2 frases.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usar como API local</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:11434/api/generate</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"model\": \"qwen2.5:7b\", \"prompt\": \"Resume qu√© es paged attention\"}'</span></span></code></pre>\n<p><strong>Regla mental de memoria (aprox.):</strong></p>\n<ul>\n<li>7B cuantizado (Q4 / Q5 / MXFP4): 4‚Äì6 GB RAM</li>\n<li>14B: 8‚Äì10 GB</li>\n<li>70B: impr√°ctico local sin GPU grande</li>\n</ul>\n<blockquote>\n<p>‚ö†Ô∏è Si tu m√°quina empieza a swappear, baja de tama√±o o usa una cuantizaci√≥n m√°s agresiva.</p>\n</blockquote>\n<hr>\n<h2 id=\"2--lm-studio-observabilidad-y-control-visual\">2. üé® LM Studio: Observabilidad y Control Visual</h2>\n<p><strong>Cu√°ndo usarlo:</strong> demos a stakeholders, afinaci√≥n de prompts, medir tokens/s, probar m√∫ltiples modelos y configuraciones de sampling.</p>\n<p><strong>Highlights:</strong></p>\n<ul>\n<li>UI amigable para gestionar descargas desde Hugging Face</li>\n<li>M√©tricas en tiempo real: velocidad (tokens/s), utilizaci√≥n</li>\n<li>Panel para par√°metros: temperature, top_p, repeat penalty</li>\n<li>Puede levantar un <strong>servidor local</strong> compatible con OpenAI API</li>\n<li>Exporta y reutiliza prompts / sesiones</li>\n</ul>\n<p><strong>Ejecutar un servidor OpenAI-like:</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\"><code><span class=\"line\"><span style=\"color:#6A737D\"># En la UI activas: Enable Local Server</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Luego puedes consumirlo desde tu app:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">export</span><span style=\"color:#E1E4E8\"> OPENAI_API_BASE</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">http://localhost:1234/v1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">export</span><span style=\"color:#E1E4E8\"> OPENAI_API_KEY</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">sk-local</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#E1E4E8\"> $OPENAI_API_BASE</span><span style=\"color:#9ECBFF\">/chat/completions</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"Authorization: Bearer </span><span style=\"color:#E1E4E8\">$OPENAI_API_KEY</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   \"model\": \"qwen2.5:7b-instruct\",</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   \"messages\": [{\"role\": \"user\", \"content\": \"Dame 3 ventajas de Go\"}],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   \"temperature\": 0.7</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"> }'</span></span></code></pre>\n<p><strong>Rendimiento observado (ejemplo orientativo):</strong></p>\n<ul>\n<li>MacBook M‚Äëseries con suficiente RAM puede lograr <strong>40‚Äì60 tokens/s</strong> con modelos 7B cuantizados MXFP4.</li>\n</ul>\n<p><strong>Cu√°ndo migrar m√°s all√° de LM Studio:</strong> cuando necesitas <strong>> concurrent users</strong>, batching, throughput estable o reducir coste por token en producci√≥n.</p>\n<hr>\n<h2 id=\"3--vllm--infra-serverless-camino-a-producci√≥n\">3. ‚ö° vLLM + Infra Serverless: Camino a Producci√≥n</h2>\n<p><strong>Objetivo:</strong> servir modelos con <strong>bajo coste amortizado</strong>, alta concurrencia y latencias predecibles.</p>\n<p><strong>¬øPor qu√© vLLM?</strong></p>\n<ul>\n<li>Motor de inferencia optimizado (PagedAttention) ‚ûú mejor utilizaci√≥n de VRAM</li>\n<li>Batching din√°mico y speculative decoding (si activado)</li>\n<li>Manejo eficiente de long context windows</li>\n<li>Soporte OpenAI-compatible endpoints (<code>/v1/completions</code>, <code>/v1/chat/completions</code>)</li>\n</ul>\n<p><strong>Arquitectura serverless t√≠pica (Runpod / Modal / Lambda GPU-like providers):</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>Cliente ‚Üí API Gateway ‚Üí (Queue) ‚Üí Worker GPU (vLLM Engine) ‚Üí Respuesta</span></span>\n<span class=\"line\"><span>                     ‚Üò Autoscaler ‚Üó</span></span></code></pre>\n<p><strong>Componentes Clave:</strong></p>\n<ul>\n<li>Queue / dispatcher (async) para nivelar bursts</li>\n<li>M√∫ltiples r√©plicas vLLM detr√°s de un load balancer</li>\n<li>M√©tricas: tokens/s, time-to-first-token, GPU utilization, rejection rate</li>\n<li>Autoscaling por cola pendiente + GPU busy ratio</li>\n</ul>\n<p><strong>Ejemplo de arranque vLLM (CLI):</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> vllm.entrypoints.openai.api_server</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --model</span><span style=\"color:#9ECBFF\"> Qwen/Qwen2.5-7B-Instruct</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --tensor-parallel-size</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --max-model-len</span><span style=\"color:#79B8FF\"> 32768</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --port</span><span style=\"color:#79B8FF\"> 8000</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --dtype</span><span style=\"color:#9ECBFF\"> auto</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --enforce-eager</span></span></code></pre>\n<p><strong>Consumir el endpoint:</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\"><code><span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8000/v1/chat/completions</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"model\": \"Qwen2.5-7B-Instruct\",</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"messages\": [{\"role\": \"user\", \"content\": \"Resume las ventajas de paged attention\"}],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"temperature\": 0.3</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"> }'</span></span></code></pre>\n<h3 id=\"-ejemplo-docker-simplificado\">üì¶ Ejemplo Docker (simplificado)</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04</span></span>\n<span class=\"line\"><span>RUN apt-get update &#x26;&#x26; apt-get install -y python3-pip git &#x26;&#x26; rm -rf /var/lib/apt/lists/*</span></span>\n<span class=\"line\"><span>RUN pip install --no-cache-dir vllm==0.5.3</span></span>\n<span class=\"line\"><span>EXPOSE 8000</span></span>\n<span class=\"line\"><span>CMD [\"python\", \"-m\", \"vllm.entrypoints.openai.api_server\", \"--model\", \"Qwen/Qwen2.5-7B-Instruct\", \"--port\", \"8000\"]</span></span></code></pre>\n<h3 id=\"-autoscaling-idea-conceptual\">üîÅ Autoscaling (idea conceptual)</h3>\n<ul>\n<li>M√©trica primaria: <code>pending_requests</code> + <code>avg_time_to_first_token</code></li>\n<li>Regla: si <code>pending_requests > X</code> durante Y segundos ‚ûú scale out</li>\n<li>Regla: si <code>gpu_utilization &#x3C; 25%</code> y <code>pending_requests == 0</code> por Z segundos ‚ûú scale in</li>\n</ul>\n<hr>\n<h2 id=\"-cuantizaci√≥n-mxfp4-y-otras\">üîß Cuantizaci√≥n (MXFP4 y otras)</h2>\n<p><strong>Por qu√© importa:</strong> reduce VRAM/RAM y coste operativo manteniendo calidad razonable.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Formato</th><th>Uso Memoria</th><th>Calidad</th><th>Caso T√≠pico</th></tr></thead><tbody><tr><td>FP16</td><td>Alto</td><td>M√°xima</td><td>Fine-tuning / evaluaci√≥n</td></tr><tr><td>BF16</td><td>Similar FP16</td><td>Estable</td><td>GPUs modernas</td></tr><tr><td>INT8</td><td>Medio</td><td>Buena</td><td>Prototipos</td></tr><tr><td>Q4_K / Q5_K</td><td>Bajo</td><td>Aceptable</td><td>Uso local</td></tr><tr><td><strong>MXFP4</strong></td><td>Muy bajo</td><td>Sorprendentemente alta</td><td>Producci√≥n coste-eficiente</td></tr></tbody></table>\n<blockquote>\n<p>Ajusta <code>max_model_len</code> y <code>max_num_seqs</code> para evitar OOM en contextos largos.</p>\n</blockquote>\n<h3 id=\"-qu√©-es-mxfp4\">üß¨ ¬øQu√© es MXFP4?</h3>\n<p><strong>MXFP4 (Mixed FP4)</strong> es una familia de esquemas de 4 bits en formato flotante mixto (no estrictamente entero) que aprovecha mini‚Äëformatos FP4 (variantes como E2M1 / E3M0) y heur√≠sticas de escalado por bloque para mantener <strong>m√°s rango din√°mico</strong> que INT4 puro, reduciendo la p√©rdida de calidad t√≠pica de quantizaciones agresivas.</p>\n<p>Caracter√≠sticas clave:</p>\n<ul>\n<li>Mezcla (‚Äúmixed‚Äù) de sub‚Äëformatos para adaptar rango vs precisi√≥n seg√∫n la distribuci√≥n interna de pesos.</li>\n<li>Normalizaci√≥n / escala por bloque (8, 16, 32, 64 par√°metros) ‚Üí reduce outliers.</li>\n<li>Puede combinarse con dequant on‚Äëthe‚Äëfly + kernels fusionados para minimizar overhead.</li>\n<li>En inference: los pesos permanecen comprimidos hasta la etapa de multiplicaci√≥n (streaming decomp).</li>\n</ul>\n<p>Comparaci√≥n (aprox.) memoria pesos (sin KV cache) para un modelo 7B:</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Representaci√≥n</th><th>Memoria ‚âà</th></tr></thead><tbody><tr><td>FP16</td><td>~13.5‚Äì14 GB</td></tr><tr><td>INT8 (per‚Äëtensor)</td><td>~7.0 GB</td></tr><tr><td>INT8 (per‚Äëchannel)</td><td>~7.5 GB (ligero overhead metadatos)</td></tr><tr><td>Q4_K (LLM.int4)</td><td>~3.8‚Äì4.2 GB</td></tr><tr><td><strong>MXFP4</strong></td><td>~3.5‚Äì3.9 GB</td></tr></tbody></table>\n<blockquote>\n<p>Las cifras var√≠an por: embedding sharing, vocab size, formato KV, capas MoE, etc.</p>\n</blockquote>\n<h3 id=\"-m√©tricas-de-calidad\">üß™ M√©tricas de Calidad</h3>\n<p>Una forma com√∫n de evaluar la degradaci√≥n es la <strong>p√©rdida de perplexity</strong> (ŒîPPL) vs FP16 y la ca√≠da en benchmarks (MMLU, GSM8K‚Ä¶). MXFP4 bien calibrado suele mostrar:</p>\n<ul>\n<li>ŒîPPL t√≠pica: +0.5 a +1.2 sobre FP16 (menor que algunos INT4 uniformes)</li>\n<li>Ca√≠da en tareas razonamiento: 0‚Äì2 puntos porcentuales (depende del modelo)</li>\n</ul>\n<h3 id=\"-gptoss-con-mxfp4\">üß† GPT‚ÄëOSS con MXFP4</h3>\n<p>Cuando veas variantes tipo <code>gpt-oss-*</code> o <code>*-mxfp4</code> normalmente indica:</p>\n<ul>\n<li><strong>Pesos base</strong> convertidos a formato MXFP4 post‚Äëtraining (PTQ) usando calibraci√≥n con subconjunto de datos.</li>\n<li>Bloques con escalado adaptativo para minimizar saturaci√≥n en capas sensibles (attention / layer norm adjacente).</li>\n<li>A veces mezcla: embedding en INT8 / capas cr√≠ticas en FP8 / resto FP4 (‚Äúmixed hybrid‚Äù).</li>\n</ul>\n<p>Beneficios pr√°cticos:</p>\n<ul>\n<li>Despliegas un modelo 7B en una GPU de 8‚Äì10GB manteniendo contexto ampliado (32K) sin OOM.</li>\n<li>Mayor throughput (menos tr√°fico de memoria) ‚Üí m√°s tokens/seg en vLLM.</li>\n<li>Coste por token inferior en entornos serverless.</li>\n</ul>\n<p>Trade‚Äëoffs:</p>\n<ul>\n<li>Fine‚Äëtuning adicional (LoRA) encima de MXFP4 puede degradar si no re‚Äëcuantizas capas tocadas.</li>\n<li>Leve incremento de error de rounding en respuestas largas (>4K tokens) ‚Üí m√°s ‚Äúdrift‚Äù.</li>\n<li>M√°s sensibilidad a prompts con muchas cifras / formatos estructurados (JSON estricto).</li>\n</ul>\n<h3 id=\"-estrategias-de-quantizaci√≥n-complementarias\">üß© Estrategias de Quantizaci√≥n Complementarias</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Estrategia</th><th>Qu√© Cuantiza</th><th>Beneficio</th><th>Riesgo</th></tr></thead><tbody><tr><td>Pesos (W)</td><td>Matrices de pesos</td><td>Ahorro VRAM principal</td><td>P√©rdida base si agresivo</td></tr><tr><td>Activaciones (A)</td><td>Output intermedio</td><td>Reduce picos VRAM</td><td>Puede introducir ruido acumulado</td></tr><tr><td>KV Cache</td><td>Claves/Valores atenci√≥n</td><td>Escala long context barato</td><td>Latencia extra por (de)quant</td></tr><tr><td>Mixta (W + KV)</td><td>Combinado</td><td>M√°ximo ahorro</td><td>Complejidad en kernels</td></tr></tbody></table>\n<h3 id=\"-f√≥rmula-aproximada-memoria-inference\">üßÆ F√≥rmula Aproximada Memoria (Inference)</h3>\n<p><code>Mem_Total ‚âà Mem_Pesos + (Batch * 2 * Layer * Head * Dim_Head * Bits_KV/8)</code></p>\n<p>Al usar MXFP4 s√≥lo en pesos (y mantener KV en FP16 / FP8), el crecimiento lineal del KV sigue dominando en contextos largos. Puedes aplicar KV INT8 / FP8 para extender contexto sin duplicar coste.</p>\n<h3 id=\"Ô∏è-ejemplos-de-uso\">‚öôÔ∏è Ejemplos de Uso</h3>\n<p>vLLM (cuando soporte el flag espec√≠fico, hoy se hace v√≠a pesos ya cuantizados):</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> vllm.entrypoints.openai.api_server</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --model</span><span style=\"color:#9ECBFF\"> your-org/gpt-oss-7b-mxfp4</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --max-model-len</span><span style=\"color:#79B8FF\"> 32768</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --tensor-parallel-size</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --enforce-eager</span></span></code></pre>\n<p>Transformers + bitsandbytes (4bit baseline) como alternativa si no hay MXFP4 nativo:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AutoModelForCausalLM, AutoTokenizer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"your-org/gpt-oss-7b-mxfp4\"</span><span style=\"color:#6A737D\">  # si expone weights ya empaquetados</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tok </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AutoTokenizer.from_pretrained(model_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AutoModelForCausalLM.from_pretrained(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    device_map</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"auto\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    load_in_4bit</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,          </span><span style=\"color:#6A737D\"># fallback INT4 si no hay kernel MXFP4</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    bnb_4bit_compute_dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"bfloat16\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    bnb_4bit_quant_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"nf4\"</span><span style=\"color:#6A737D\">   # nf4 se aproxima cuando MXFP4 no disponible</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span></code></pre>\n<p>KV Cache cuantizado (ejemplo conceptual en vLLM / configuraciones emergentes):</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\"><code><span class=\"line\"><span style=\"color:#B392F0\">--quantization-config</span><span style=\"color:#9ECBFF\"> '{\"kv_cache_dtype\": \"fp8\", \"weight_format\": \"mxfp4\"}'</span></span></code></pre>\n<h3 id=\"-pitfalls-espec√≠ficos-mxfp4\">üõë Pitfalls Espec√≠ficos MXFP4</h3>\n<ul>\n<li>Reconvertir a FP16 para mezclar LoRA + MXFP4 puede invalidar la compresi√≥n (cadenas de conversi√≥n ‚Üí p√©rdida).</li>\n<li>Benchmarks cortos (short prompts) pueden ocultar degradaci√≥n en razonamiento largo.</li>\n<li>M√©tricas de calidad deben revisarse por dominio (code vs general chat).</li>\n</ul>\n<h3 id=\"-buenas-pr√°cticas\">‚úÖ Buenas Pr√°cticas</h3>\n<ul>\n<li>Valida con un set de prompts cr√≠ticos (JSON, c√≥digo, reasoning, multi‚Äëturn).</li>\n<li>Ajusta <code>max_model_len</code> antes de escalar horizontalmente ‚Äî reduce costo inmediato.</li>\n<li>Monitorea <code>tokens/s</code> y <code>time_to_first_token</code> tras introducir quantizaci√≥n; deber√≠an mejorar o al menos no empeorar significativamente.</li>\n<li>Documenta el formato exacto (MXFP4 variante) para reproducibilidad.</li>\n</ul>\n<hr>\n<hr>\n<h2 id=\"-benchmarks-mentales-orientativos\">üß™ Benchmarks Mentales (Orientativos)</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Modelo (7B)</th><th>Entorno</th><th>Quant</th><th>Tokens/s (‚âà)</th><th>Notas</th></tr></thead><tbody><tr><td>Qwen 2.5</td><td>Ollama local</td><td>Q4_K</td><td>25‚Äì35</td><td>Laptop M-series</td></tr><tr><td>Qwen 2.5</td><td>LM Studio</td><td>MXFP4</td><td>40‚Äì60</td><td>Mac M4 Pro 48GB</td></tr><tr><td>Qwen 2.5</td><td>vLLM H100</td><td>MXFP4</td><td>180‚Äì250</td><td>Batching 8 req</td></tr></tbody></table>\n<p><em>No son cifras oficiales; sirven para ordenar magnitudes.</em></p>\n<hr>\n<h2 id=\"-c√≥mo-elegir\">üß≠ C√≥mo Elegir</h2>\n<ol>\n<li>¬øEst√°s aprendiendo? ‚ûú <strong>Ollama</strong></li>\n<li>¬øNecesitas mostrar algo visual / iterar prompts con m√©tricas? ‚ûú <strong>LM Studio</strong></li>\n<li>¬øTienes usuarios concurrentes / SLA / costo por token controlado? ‚ûú <strong>vLLM serverless</strong></li>\n</ol>\n<h3 id=\"checklist-producci√≥n-vllm\">Checklist Producci√≥n (vLLM)</h3>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> Limitar <code>max_model_len</code> (ej: 32K vs 256K si no lo necesitas)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> Activar logging estructurado</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> M√©tricas: TTFB, tokens/s, cola, errores</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> Health checks para readiness / liveness</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> Estrategia de rotaci√≥n de versiones de modelo</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> Pol√≠tica de escalado (fr√≠o/caliente)</li>\n</ul>\n<hr>\n<h2 id=\"-cost--optimizaci√≥n\">üí∞ Cost &#x26; Optimizaci√≥n</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Palanca</th><th>Impacto</th><th>Acci√≥n</th></tr></thead><tbody><tr><td>Cuantizaci√≥n</td><td>‚Üì VRAM</td><td>MXFP4 / INT8</td></tr><tr><td>Contexto</td><td>‚Üì Memoria &#x26; Latencia</td><td>Limita a 32K si 256K no aporta</td></tr><tr><td>Batching</td><td>‚Üë Throughput</td><td>Ajusta tama√±o din√°mico</td></tr><tr><td>Streaming</td><td>‚Üì Percepci√≥n de latencia</td><td>Env√≠a tokens tempranos</td></tr><tr><td>Warm Pools</td><td>‚Üì Cold start</td><td>Mant√©n 1‚ÄìN r√©plicas calientes</td></tr></tbody></table>\n<hr>\n<h2 id=\"Ô∏è-pitfalls-comunes\">‚ö†Ô∏è Pitfalls Comunes</h2>\n<ul>\n<li>Subir un contexto gigantesco para prompts simples</li>\n<li>No medir <code>time_to_first_token</code> (percepci√≥n usuario)</li>\n<li>Hacer scale-out sin l√≠mite ‚ûú costo explosivo nocturno</li>\n<li>Usar modelos demasiado grandes para casos que caben en 7B</li>\n<li>Ignorar l√≠mites de cuota del proveedor serverless</li>\n</ul>\n<hr>\n<h2 id=\"-el-futuro-cercano\">üîÆ El Futuro Cercano</h2>\n<ul>\n<li>M√°s <strong>cuantizaciones h√≠bridas</strong> manteniendo calidad casi FP16</li>\n<li><strong>Inference fusion</strong> (operadores combinados) por defecto</li>\n<li><strong>Multi-tenancy</strong> nativa m√°s segura en motores de inferencia</li>\n<li>Modelos 7B cada vez m√°s competitivos ‚Üí drenan necesidad de 70B</li>\n<li>Edge + on-device acelerado (NPUs / GPUs integradas)</li>\n</ul>\n<hr>\n<h2 id=\"Ô∏è-snippet-comparativo-mismo-prompt\">üõ†Ô∏è Snippet Comparativo (Mismo Prompt)</h2>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Ollama</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">ollama</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> qwen2.5:7b</span><span style=\"color:#9ECBFF\"> \"Explica en una frase qu√© es paged attention\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># LM Studio (OpenAI compatible)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#E1E4E8\"> $OPENAI_API_BASE</span><span style=\"color:#9ECBFF\">/chat/completions</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"Authorization: Bearer </span><span style=\"color:#E1E4E8\">$OPENAI_API_KEY</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '{\"model\":\"qwen2.5-7b\",\"messages\":[{\"role\":\"user\",\"content\":\"Explica en una frase qu√© es paged attention\"}]}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># vLLM</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8000/v1/chat/completions</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '{\"model\":\"Qwen2.5-7B-Instruct\",\"messages\":[{\"role\":\"user\",\"content\":\"Explica en una frase qu√© es paged attention\"}],\"stream\":true}'</span></span></code></pre>\n<hr>\n<h2 id=\"-conclusi√≥n\">‚úÖ Conclusi√≥n</h2>\n<p>No existe una √∫nica ‚Äúmejor‚Äù forma de ejecutar LLMs. <strong>Existe la correcta para tu fase actual.</strong> Empieza simple, gana comprensi√≥n, a√±ade observabilidad y termina escalando con un motor preparado para producci√≥n.</p>\n<p>¬øYa probaste alguna? ¬øPlaneas migrar a vLLM? Cu√©ntamelo y podemos extender esta gu√≠a con benchmarks reales.</p>\n<hr>\n<p>#Ô∏è‚É£ <strong>#LLM #AI #Serverless #vLLM #Ollama #LMStudio #MLOps #Infraestructura #OpenSource</strong></p>";

				const frontmatter = {"layout":"../../layouts/BlogLayout.astro","title":"3 Formas de Ejecutar Modelos LLM Open Source","description":"Gu√≠a pr√°ctica: pruebas locales con Ollama, GUI con LM Studio y despliegues serverless escalables con vLLM en infraestructura on-demand.","tags":["LLM","AI","Serverless","vLLM","Ollama","LM Studio","MLOps","Infraestructura"],"time":"12 min read","timestamp":"2025-08-22T10:00:00.000Z","featured":false,"filename":"serverless-llm-options"};
				const file = "/Users/devjaime/Documents/devjaimeblog/src/pages/blog/2025-08-22_serverless-llm-options.md";
				const url = "/blog/2025-08-22_serverless-llm-options";
				function rawContent() {
					return "   \n                                      \n                                                     \n                                                                                                                                                      \n                                                                                            \n                 \n                                    \n               \n                                \n   \n\n# 3 Formas de Ejecutar Modelos LLM Open Source: de Local a Serverless\n\n*Ollama ¬∑ LM Studio ¬∑ vLLM (serverless con Runpod / infraestructura on‚Äëdemand)*\n\nLa adopci√≥n de modelos open source (Qwen, Llama 3, DeepSeek, Mistral, etc.) est√° explotando. Ya no dependes exclusivamente de APIs cerradas: puedes iterar localmente, empaquetar prototipos y finalmente escalar a producci√≥n **sin bloquearte a un proveedor**.\n\nEn este post te cuento **tres enfoques complementarios** para ejecutar LLMs seg√∫n tu etapa: experimentaci√≥n, prototipado avanzado y producci√≥n serverless.\n\n## üéØ Resumen R√°pido\n\n| Caso de uso | Herramienta | Ventajas Clave | Trade-offs |\n|-------------|-------------|----------------|------------|\n| Aprender / Probar | Ollama | Instalaci√≥n m√≠nima, modelos cuantizados, CLI simple | Menos m√©tricas y tooling visual |\n| Prototipar / Demo GUI | LM Studio | Interfaz gr√°fica, monitor de tokens/s, servidor HTTP, selecci√≥n HF | Requiere m√°s RAM/VRAM para fluidez |\n| Producci√≥n escalable | vLLM + infraestructura serverless (Runpod, Modal, etc.) | Throughput alto, batching, paged attention, escalado horizontal | Curva de configuraci√≥n, costo si mal dimensionado |\n\n---\n## 1. üñ•Ô∏è Ollama: La Puerta de Entrada Simplicidad ‚ûú Iteraci√≥n Inmediata\n\n**Cu√°ndo usarlo:** primeras pruebas, evaluar distintos modelos, fine-tuning ligero (con extensiones), construir scripts locales.\n\n**Ventajas:**\n- Instalaci√≥n en segundos (Mac/Linux/Windows WSL)\n- Descarga y gesti√≥n de modelos unificada (`ollama pull`)\n- Soporte para cuantizaciones que bajan el consumo de RAM/VRAM\n- Sirve un endpoint local (`/api/generate`) para integraciones r√°pidas\n\n**Ejemplo b√°sico:**\n```bash\n# Descargar y ejecutar (streaming en consola)\nollama run qwen2.5:7b\n\n# Promptea directamente\n> Eres un experto en Go. Explica qu√© es un goroutine en 2 frases.\n\n# Usar como API local\ncurl http://localhost:11434/api/generate \\\n  -d '{\"model\": \"qwen2.5:7b\", \"prompt\": \"Resume qu√© es paged attention\"}'\n```\n\n**Regla mental de memoria (aprox.):**\n- 7B cuantizado (Q4 / Q5 / MXFP4): 4‚Äì6 GB RAM\n- 14B: 8‚Äì10 GB\n- 70B: impr√°ctico local sin GPU grande\n\n> ‚ö†Ô∏è Si tu m√°quina empieza a swappear, baja de tama√±o o usa una cuantizaci√≥n m√°s agresiva.\n\n---\n## 2. üé® LM Studio: Observabilidad y Control Visual\n\n**Cu√°ndo usarlo:** demos a stakeholders, afinaci√≥n de prompts, medir tokens/s, probar m√∫ltiples modelos y configuraciones de sampling.\n\n**Highlights:**\n- UI amigable para gestionar descargas desde Hugging Face\n- M√©tricas en tiempo real: velocidad (tokens/s), utilizaci√≥n\n- Panel para par√°metros: temperature, top_p, repeat penalty\n- Puede levantar un **servidor local** compatible con OpenAI API\n- Exporta y reutiliza prompts / sesiones\n\n**Ejecutar un servidor OpenAI-like:**\n```bash\n# En la UI activas: Enable Local Server\n# Luego puedes consumirlo desde tu app:\nexport OPENAI_API_BASE=http://localhost:1234/v1\nexport OPENAI_API_KEY=sk-local\n\ncurl $OPENAI_API_BASE/chat/completions \\\n -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n -H \"Content-Type: application/json\" \\\n -d '{\n   \"model\": \"qwen2.5:7b-instruct\",\n   \"messages\": [{\"role\": \"user\", \"content\": \"Dame 3 ventajas de Go\"}],\n   \"temperature\": 0.7\n }'\n```\n\n**Rendimiento observado (ejemplo orientativo):**\n- MacBook M‚Äëseries con suficiente RAM puede lograr **40‚Äì60 tokens/s** con modelos 7B cuantizados MXFP4.\n\n**Cu√°ndo migrar m√°s all√° de LM Studio:** cuando necesitas **> concurrent users**, batching, throughput estable o reducir coste por token en producci√≥n.\n\n---\n## 3. ‚ö° vLLM + Infra Serverless: Camino a Producci√≥n\n\n**Objetivo:** servir modelos con **bajo coste amortizado**, alta concurrencia y latencias predecibles.\n\n**¬øPor qu√© vLLM?**\n- Motor de inferencia optimizado (PagedAttention) ‚ûú mejor utilizaci√≥n de VRAM\n- Batching din√°mico y speculative decoding (si activado)\n- Manejo eficiente de long context windows\n- Soporte OpenAI-compatible endpoints (`/v1/completions`, `/v1/chat/completions`)\n\n**Arquitectura serverless t√≠pica (Runpod / Modal / Lambda GPU-like providers):**\n```\nCliente ‚Üí API Gateway ‚Üí (Queue) ‚Üí Worker GPU (vLLM Engine) ‚Üí Respuesta\n                     ‚Üò Autoscaler ‚Üó\n```\n\n**Componentes Clave:**\n- Queue / dispatcher (async) para nivelar bursts\n- M√∫ltiples r√©plicas vLLM detr√°s de un load balancer\n- M√©tricas: tokens/s, time-to-first-token, GPU utilization, rejection rate\n- Autoscaling por cola pendiente + GPU busy ratio\n\n**Ejemplo de arranque vLLM (CLI):**\n```bash\npython -m vllm.entrypoints.openai.api_server \\\n  --model Qwen/Qwen2.5-7B-Instruct \\\n  --tensor-parallel-size 1 \\\n  --max-model-len 32768 \\\n  --port 8000 \\\n  --dtype auto \\\n  --enforce-eager\n```\n\n**Consumir el endpoint:**\n```bash\ncurl http://localhost:8000/v1/chat/completions \\\n -H \"Content-Type: application/json\" \\\n -d '{\n  \"model\": \"Qwen2.5-7B-Instruct\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Resume las ventajas de paged attention\"}],\n  \"temperature\": 0.3\n }'\n```\n\n### üì¶ Ejemplo Docker (simplificado)\n```Dockerfile\nFROM nvidia/cuda:12.1.0-runtime-ubuntu22.04\nRUN apt-get update && apt-get install -y python3-pip git && rm -rf /var/lib/apt/lists/*\nRUN pip install --no-cache-dir vllm==0.5.3\nEXPOSE 8000\nCMD [\"python\", \"-m\", \"vllm.entrypoints.openai.api_server\", \"--model\", \"Qwen/Qwen2.5-7B-Instruct\", \"--port\", \"8000\"]\n```\n\n### üîÅ Autoscaling (idea conceptual)\n- M√©trica primaria: `pending_requests` + `avg_time_to_first_token`\n- Regla: si `pending_requests > X` durante Y segundos ‚ûú scale out\n- Regla: si `gpu_utilization < 25%` y `pending_requests == 0` por Z segundos ‚ûú scale in\n\n---\n## üîß Cuantizaci√≥n (MXFP4 y otras)\n\n**Por qu√© importa:** reduce VRAM/RAM y coste operativo manteniendo calidad razonable.\n\n| Formato | Uso Memoria | Calidad | Caso T√≠pico |\n|---------|-------------|---------|-------------|\n| FP16 | Alto | M√°xima | Fine-tuning / evaluaci√≥n\n| BF16 | Similar FP16 | Estable | GPUs modernas\n| INT8 | Medio | Buena | Prototipos\n| Q4_K / Q5_K | Bajo | Aceptable | Uso local\n| **MXFP4** | Muy bajo | Sorprendentemente alta | Producci√≥n coste-eficiente\n\n> Ajusta `max_model_len` y `max_num_seqs` para evitar OOM en contextos largos.\n\n### üß¨ ¬øQu√© es MXFP4?\n\n**MXFP4 (Mixed FP4)** es una familia de esquemas de 4 bits en formato flotante mixto (no estrictamente entero) que aprovecha mini‚Äëformatos FP4 (variantes como E2M1 / E3M0) y heur√≠sticas de escalado por bloque para mantener **m√°s rango din√°mico** que INT4 puro, reduciendo la p√©rdida de calidad t√≠pica de quantizaciones agresivas.\n\nCaracter√≠sticas clave:\n* Mezcla (\"mixed\") de sub‚Äëformatos para adaptar rango vs precisi√≥n seg√∫n la distribuci√≥n interna de pesos.\n* Normalizaci√≥n / escala por bloque (8, 16, 32, 64 par√°metros) ‚Üí reduce outliers.\n* Puede combinarse con dequant on‚Äëthe‚Äëfly + kernels fusionados para minimizar overhead.\n* En inference: los pesos permanecen comprimidos hasta la etapa de multiplicaci√≥n (streaming decomp).\n\nComparaci√≥n (aprox.) memoria pesos (sin KV cache) para un modelo 7B:\n| Representaci√≥n | Memoria ‚âà |\n|----------------|-----------|\n| FP16 | ~13.5‚Äì14 GB |\n| INT8 (per‚Äëtensor) | ~7.0 GB |\n| INT8 (per‚Äëchannel) | ~7.5 GB (ligero overhead metadatos) |\n| Q4_K (LLM.int4) | ~3.8‚Äì4.2 GB |\n| **MXFP4** | ~3.5‚Äì3.9 GB |\n\n> Las cifras var√≠an por: embedding sharing, vocab size, formato KV, capas MoE, etc.\n\n### üß™ M√©tricas de Calidad\n\nUna forma com√∫n de evaluar la degradaci√≥n es la **p√©rdida de perplexity** (ŒîPPL) vs FP16 y la ca√≠da en benchmarks (MMLU, GSM8K...). MXFP4 bien calibrado suele mostrar:\n* ŒîPPL t√≠pica: +0.5 a +1.2 sobre FP16 (menor que algunos INT4 uniformes)\n* Ca√≠da en tareas razonamiento: 0‚Äì2 puntos porcentuales (depende del modelo)\n\n### üß† GPT‚ÄëOSS con MXFP4\n\nCuando veas variantes tipo `gpt-oss-*` o `*-mxfp4` normalmente indica:\n* **Pesos base** convertidos a formato MXFP4 post‚Äëtraining (PTQ) usando calibraci√≥n con subconjunto de datos.\n* Bloques con escalado adaptativo para minimizar saturaci√≥n en capas sensibles (attention / layer norm adjacente).\n* A veces mezcla: embedding en INT8 / capas cr√≠ticas en FP8 / resto FP4 (\"mixed hybrid\").\n\nBeneficios pr√°cticos:\n* Despliegas un modelo 7B en una GPU de 8‚Äì10GB manteniendo contexto ampliado (32K) sin OOM.\n* Mayor throughput (menos tr√°fico de memoria) ‚Üí m√°s tokens/seg en vLLM.\n* Coste por token inferior en entornos serverless.\n\nTrade‚Äëoffs:\n* Fine‚Äëtuning adicional (LoRA) encima de MXFP4 puede degradar si no re‚Äëcuantizas capas tocadas.\n* Leve incremento de error de rounding en respuestas largas (>4K tokens) ‚Üí m√°s \"drift\".\n* M√°s sensibilidad a prompts con muchas cifras / formatos estructurados (JSON estricto).\n\n### üß© Estrategias de Quantizaci√≥n Complementarias\n\n| Estrategia | Qu√© Cuantiza | Beneficio | Riesgo |\n|------------|--------------|-----------|--------|\n| Pesos (W) | Matrices de pesos | Ahorro VRAM principal | P√©rdida base si agresivo |\n| Activaciones (A) | Output intermedio | Reduce picos VRAM | Puede introducir ruido acumulado |\n| KV Cache | Claves/Valores atenci√≥n | Escala long context barato | Latencia extra por (de)quant |\n| Mixta (W + KV) | Combinado | M√°ximo ahorro | Complejidad en kernels |\n\n### üßÆ F√≥rmula Aproximada Memoria (Inference)\n\n`Mem_Total ‚âà Mem_Pesos + (Batch * 2 * Layer * Head * Dim_Head * Bits_KV/8)`\n\nAl usar MXFP4 s√≥lo en pesos (y mantener KV en FP16 / FP8), el crecimiento lineal del KV sigue dominando en contextos largos. Puedes aplicar KV INT8 / FP8 para extender contexto sin duplicar coste.\n\n### ‚öôÔ∏è Ejemplos de Uso\n\nvLLM (cuando soporte el flag espec√≠fico, hoy se hace v√≠a pesos ya cuantizados):\n```bash\npython -m vllm.entrypoints.openai.api_server \\\n  --model your-org/gpt-oss-7b-mxfp4 \\\n  --max-model-len 32768 \\\n  --tensor-parallel-size 1 \\\n  --enforce-eager\n```\n\nTransformers + bitsandbytes (4bit baseline) como alternativa si no hay MXFP4 nativo:\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"your-org/gpt-oss-7b-mxfp4\"  # si expone weights ya empaquetados\ntok = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    load_in_4bit=True,          # fallback INT4 si no hay kernel MXFP4\n    bnb_4bit_compute_dtype=\"bfloat16\",\n    bnb_4bit_quant_type=\"nf4\"   # nf4 se aproxima cuando MXFP4 no disponible\n)\n```\n\nKV Cache cuantizado (ejemplo conceptual en vLLM / configuraciones emergentes):\n```bash\n--quantization-config '{\"kv_cache_dtype\": \"fp8\", \"weight_format\": \"mxfp4\"}'\n```\n\n### üõë Pitfalls Espec√≠ficos MXFP4\n* Reconvertir a FP16 para mezclar LoRA + MXFP4 puede invalidar la compresi√≥n (cadenas de conversi√≥n ‚Üí p√©rdida).\n* Benchmarks cortos (short prompts) pueden ocultar degradaci√≥n en razonamiento largo.\n* M√©tricas de calidad deben revisarse por dominio (code vs general chat).\n\n### ‚úÖ Buenas Pr√°cticas\n* Valida con un set de prompts cr√≠ticos (JSON, c√≥digo, reasoning, multi‚Äëturn).\n* Ajusta `max_model_len` antes de escalar horizontalmente ‚Äî reduce costo inmediato.\n* Monitorea `tokens/s` y `time_to_first_token` tras introducir quantizaci√≥n; deber√≠an mejorar o al menos no empeorar significativamente.\n* Documenta el formato exacto (MXFP4 variante) para reproducibilidad.\n\n---\n\n---\n## üß™ Benchmarks Mentales (Orientativos)\n\n| Modelo (7B) | Entorno | Quant | Tokens/s (‚âà) | Notas |\n|-------------|---------|-------|--------------|-------|\n| Qwen 2.5 | Ollama local | Q4_K | 25‚Äì35 | Laptop M-series |\n| Qwen 2.5 | LM Studio | MXFP4 | 40‚Äì60 | Mac M4 Pro 48GB |\n| Qwen 2.5 | vLLM H100 | MXFP4 | 180‚Äì250 | Batching 8 req |\n\n*No son cifras oficiales; sirven para ordenar magnitudes.*\n\n---\n## üß≠ C√≥mo Elegir\n\n1. ¬øEst√°s aprendiendo? ‚ûú **Ollama**\n2. ¬øNecesitas mostrar algo visual / iterar prompts con m√©tricas? ‚ûú **LM Studio**\n3. ¬øTienes usuarios concurrentes / SLA / costo por token controlado? ‚ûú **vLLM serverless**\n\n### Checklist Producci√≥n (vLLM)\n- [ ] Limitar `max_model_len` (ej: 32K vs 256K si no lo necesitas)\n- [ ] Activar logging estructurado\n- [ ] M√©tricas: TTFB, tokens/s, cola, errores\n- [ ] Health checks para readiness / liveness\n- [ ] Estrategia de rotaci√≥n de versiones de modelo\n- [ ] Pol√≠tica de escalado (fr√≠o/caliente)\n\n---\n## üí∞ Cost & Optimizaci√≥n\n\n| Palanca | Impacto | Acci√≥n |\n|---------|---------|--------|\n| Cuantizaci√≥n | ‚Üì VRAM | MXFP4 / INT8 | \n| Contexto | ‚Üì Memoria & Latencia | Limita a 32K si 256K no aporta |\n| Batching | ‚Üë Throughput | Ajusta tama√±o din√°mico |\n| Streaming | ‚Üì Percepci√≥n de latencia | Env√≠a tokens tempranos |\n| Warm Pools | ‚Üì Cold start | Mant√©n 1‚ÄìN r√©plicas calientes |\n\n---\n## ‚ö†Ô∏è Pitfalls Comunes\n- Subir un contexto gigantesco para prompts simples\n- No medir `time_to_first_token` (percepci√≥n usuario)\n- Hacer scale-out sin l√≠mite ‚ûú costo explosivo nocturno\n- Usar modelos demasiado grandes para casos que caben en 7B\n- Ignorar l√≠mites de cuota del proveedor serverless\n\n---\n## üîÆ El Futuro Cercano\n- M√°s **cuantizaciones h√≠bridas** manteniendo calidad casi FP16\n- **Inference fusion** (operadores combinados) por defecto\n- **Multi-tenancy** nativa m√°s segura en motores de inferencia\n- Modelos 7B cada vez m√°s competitivos ‚Üí drenan necesidad de 70B\n- Edge + on-device acelerado (NPUs / GPUs integradas)\n\n---\n## üõ†Ô∏è Snippet Comparativo (Mismo Prompt)\n\n```bash\n# Ollama\nollama run qwen2.5:7b \"Explica en una frase qu√© es paged attention\"\n\n# LM Studio (OpenAI compatible)\ncurl $OPENAI_API_BASE/chat/completions \\\n -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n -H \"Content-Type: application/json\" \\\n -d '{\"model\":\"qwen2.5-7b\",\"messages\":[{\"role\":\"user\",\"content\":\"Explica en una frase qu√© es paged attention\"}]}'\n\n# vLLM\ncurl http://localhost:8000/v1/chat/completions \\\n -H \"Content-Type: application/json\" \\\n -d '{\"model\":\"Qwen2.5-7B-Instruct\",\"messages\":[{\"role\":\"user\",\"content\":\"Explica en una frase qu√© es paged attention\"}],\"stream\":true}'\n```\n\n---\n## ‚úÖ Conclusi√≥n\nNo existe una √∫nica \"mejor\" forma de ejecutar LLMs. **Existe la correcta para tu fase actual.** Empieza simple, gana comprensi√≥n, a√±ade observabilidad y termina escalando con un motor preparado para producci√≥n.\n\n¬øYa probaste alguna? ¬øPlaneas migrar a vLLM? Cu√©ntamelo y podemos extender esta gu√≠a con benchmarks reales.\n\n---\n\n#Ô∏è‚É£ **#LLM #AI #Serverless #vLLM #Ollama #LMStudio #MLOps #Infraestructura #OpenSource**\n";
				}
				async function compiledContent() {
					return await html();
				}
				function getHeadings() {
					return [{"depth":1,"slug":"3-formas-de-ejecutar-modelos-llm-open-source-de-local-a-serverless","text":"3 Formas de Ejecutar Modelos LLM Open Source: de Local a Serverless"},{"depth":2,"slug":"-resumen-r√°pido","text":"üéØ Resumen R√°pido"},{"depth":2,"slug":"1-Ô∏è-ollama-la-puerta-de-entrada-simplicidad--iteraci√≥n-inmediata","text":"1. üñ•Ô∏è Ollama: La Puerta de Entrada Simplicidad ‚ûú Iteraci√≥n Inmediata"},{"depth":2,"slug":"2--lm-studio-observabilidad-y-control-visual","text":"2. üé® LM Studio: Observabilidad y Control Visual"},{"depth":2,"slug":"3--vllm--infra-serverless-camino-a-producci√≥n","text":"3. ‚ö° vLLM + Infra Serverless: Camino a Producci√≥n"},{"depth":3,"slug":"-ejemplo-docker-simplificado","text":"üì¶ Ejemplo Docker (simplificado)"},{"depth":3,"slug":"-autoscaling-idea-conceptual","text":"üîÅ Autoscaling (idea conceptual)"},{"depth":2,"slug":"-cuantizaci√≥n-mxfp4-y-otras","text":"üîß Cuantizaci√≥n (MXFP4 y otras)"},{"depth":3,"slug":"-qu√©-es-mxfp4","text":"üß¨ ¬øQu√© es MXFP4?"},{"depth":3,"slug":"-m√©tricas-de-calidad","text":"üß™ M√©tricas de Calidad"},{"depth":3,"slug":"-gptoss-con-mxfp4","text":"üß† GPT‚ÄëOSS con MXFP4"},{"depth":3,"slug":"-estrategias-de-quantizaci√≥n-complementarias","text":"üß© Estrategias de Quantizaci√≥n Complementarias"},{"depth":3,"slug":"-f√≥rmula-aproximada-memoria-inference","text":"üßÆ F√≥rmula Aproximada Memoria (Inference)"},{"depth":3,"slug":"Ô∏è-ejemplos-de-uso","text":"‚öôÔ∏è Ejemplos de Uso"},{"depth":3,"slug":"-pitfalls-espec√≠ficos-mxfp4","text":"üõë Pitfalls Espec√≠ficos MXFP4"},{"depth":3,"slug":"-buenas-pr√°cticas","text":"‚úÖ Buenas Pr√°cticas"},{"depth":2,"slug":"-benchmarks-mentales-orientativos","text":"üß™ Benchmarks Mentales (Orientativos)"},{"depth":2,"slug":"-c√≥mo-elegir","text":"üß≠ C√≥mo Elegir"},{"depth":3,"slug":"checklist-producci√≥n-vllm","text":"Checklist Producci√≥n (vLLM)"},{"depth":2,"slug":"-cost--optimizaci√≥n","text":"üí∞ Cost & Optimizaci√≥n"},{"depth":2,"slug":"Ô∏è-pitfalls-comunes","text":"‚ö†Ô∏è Pitfalls Comunes"},{"depth":2,"slug":"-el-futuro-cercano","text":"üîÆ El Futuro Cercano"},{"depth":2,"slug":"Ô∏è-snippet-comparativo-mismo-prompt","text":"üõ†Ô∏è Snippet Comparativo (Mismo Prompt)"},{"depth":2,"slug":"-conclusi√≥n","text":"‚úÖ Conclusi√≥n"}];
				}

				const Content = createComponent((result, _props, slots) => {
					const { layout, ...content } = frontmatter;
					content.file = file;
					content.url = url;

					return renderTemplate`${renderComponent(result, 'Layout', $$BlogLayout, {
								file,
								url,
								content,
								frontmatter: content,
								headings: getHeadings(),
								rawContent,
								compiledContent,
								'server:root': true,
							}, {
								'default': () => renderTemplate`${unescapeHTML(html())}`
							})}`;
				});

const _page = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
	__proto__: null,
	Content,
	compiledContent,
	default: Content,
	file,
	frontmatter,
	getHeadings,
	rawContent,
	url
}, Symbol.toStringTag, { value: 'Module' }));

export { _page as _ };
