/* empty css                                                                    */
import { d as createComponent, i as renderComponent, r as renderTemplate, u as unescapeHTML } from './astro/server_C7nAViGe.mjs';
import 'kleur/colors';
import { $ as $$BlogLayout } from './BlogLayout_COI89YL8.mjs';

const html = () => "<h1 id=\"introducción-a-cómo-funciona-kafka-e-implementación-usando-python-client\">Introducción a cómo funciona Kafka e implementación usando Python-client</h1>\n<p>Configure un cliente de Python para Kafka con kafka-python</p>\n<hr>\n<h3 id=\"introducción-a-cómo-funciona-kafka-e-implementación-usando-python-client-1\">Introducción a cómo funciona Kafka e implementación usando Python-client</h3>\n<p><img src=\"https://cdn-images-1.medium.com/max/800/1*BqSRfLZY2b4wZ6qg1fBTuw.png\" alt=\"\"></p>\n<hr>\n<h3 id=\"configure-un-cliente-de-python-para-kafka-con-kafka-python\">Configure un cliente de Python para Kafka con kafka-python</h3>\n<p>El uso de datos en tiempo real se ha convertido en el uso comercial recurrente tanto para las empresas como para sus clientes. Sin embargo, uno de los factores clave a tener en cuenta es cómo el caso de uso comercial se basa en sus datos para el uso en tiempo real, es decir, ¿el caso de uso escribe más datos de los que lee, más de lectura que de escritura.<br>\nEs por esto que necesariamente se necesita procesar datos en tiempo real y en un enfoque basado en eventos, aquí es donde entra en juego Apache Kafka. Repasaremos qué es Kafka, los conceptos de Kafka, quién lo está usando, cómo configurarlo y cómo usarlo con un cliente Python ( <code>kafka-python</code>) en este tutorial.</p>\n<p><strong>¿Qué es Apache Kafka?</strong></p>\n<p>Kafka es un sistema de mensajería distribuida de transmisión de eventos que consta de servidores y clientes que se comunican a través del protocolo de red TCP de alto rendimiento.<br>\n.</p>\n<blockquote>\n<p>Nota: Kafka se desarrolló en Linkedin, pero ahora se administra bajo la fundación Apache, por lo tanto, Apache Kafka. Me referiré a Apache Kafka como Kafka a lo largo de este tutorial.</p>\n</blockquote>\n<p><strong>Transmisión de eventos</strong></p>\n<p>La transmisión de eventos es la captura, el procesamiento y la transformación de datos en tiempo real en varios eventos de diferentes fuentes, por ejemplo, clicks en sitios web, bases de datos, sistemas de registro, dispositivos IOT, etc.</p>\n<p>al mismo tiempo que garantiza el flujo continuo y el enrutamiento de datos de flujo a varios destinos anticipando los datos del evento.</p>\n<p><strong>¿Por qué Kafka?</strong></p>\n<p>Kafka se utiliza en arquitecturas de datos de transmisión de eventos en tiempo real para proporcionar análisis de datos, los mensajes se almacenan en el disco con Kafka, lo que proporciona replicación dentro del clúster, lo que hace que los mensajes sean más duraderos, más confiables y admitan múltiples suscriptores.</p>\n<p>Kafka puede transmitir eventos continuamente mediante el uso<br>\ndel modelo de publicación-suscripción (pub-sub) en el que los eventos se pueden leer (suscribir).</p>\n<p>Tan pronto como se escriben (publicar), procesar o incluso almacenar para la<br>\nretención de datos durante un período como Kafka da la flexibilidad sobre cuánto tiempo retener (almacenar) los datos.</p>\n<p><strong>¿Por qué Kafka es tan rápido?</strong></p>\n<p>Kafka es rápido por varias razones, destacaremos algunas de estas razones a continuación.</p>\n<ol>\n<li><strong>Copia cero</strong>: se basa en gran medida en el principio <a href=\"https://en.wikipedia.org/wiki/Zero-copy\">de copia cero,</a> es decir, interactúa directamente con el kernel del sistema operativo para mover datos.</li>\n<li><strong>Procesamiento por lotes</strong>: permite el procesamiento por lotes de datos en fragmentos, lo que permite una compresión de datos eficiente y, por lo tanto, reduce la latencia de E/S.</li>\n<li><strong>Escalado horizontal</strong>: Kafka permite el escalado horizontal, ya que permite múltiples particiones (incluso en miles) sobre un tema que podría estar en miles de máquinas, ya sea en las instalaciones o en la nube, lo que lo hace muy capaz de soportar grandes cargas.</li>\n<li><strong>Evitar la memoria RAM</strong>: Kafka escribe en un registro de compromiso inmutable en el disco secuencialmente, evitando así la búsqueda lenta del disco.</li>\n</ol>\n<p><strong>¿Qué problema resuelve Kafka?</strong></p>\n<p>Con el auge de la innovación en varios aspectos de la vida desde Internet de<br>\nlas cosas (IOT), automóviles autónomos, inteligencia artificial, soluciones de cadena de bloques, robótica y muchos más, por mencionar algunos, la tasa de generación de datos está creciendo exponencialmente y no es ralentizando en el corto plazo.</p>\n<p>Por lo tanto, para que las empresas innoven y entiendan más a sus clientes y brinden mejores servicios, la forma tradicional de desarrollo de software debe mejorarse para incorporar el flujo de este enorme y creciente conjunto de datos de varias fuentes de datos, incluidas las<br>\nmencionadas y otras. Con Kafka, todos los diversos componentes del<br>\nsistema pueden comunicarse en un enfoque basado en eventos donde un evento de una parte del sistema se traduce en acción en otra parte del<br>\nsistema, la belleza de esto es que sucederá en tiempo real.</p>\n<p><strong>¿Qué empresas usan Kafka?</strong></p>\n<p>Miles de empresas están utilizando Kafka en la producción, incluidas las<br>\nempresas como Walmart o Uber, algunas de las empresas incluyen Microsoft, Netflix, Goldman, Sachs, Target, Cisco, Intuit, Box, Pinterest, New York Times y muchas <a href=\"https://kafka.apache.org/powered-by\">más</a> .</p>\n<p><strong>Primeros pasos con Kafka.</strong></p>\n<p>Kafka implica la comunicación entre servidores y clientes.</p>\n<p><strong>Servidores</strong> : Kafka se ejecuta como un clúster de uno o más servidores que pueden estar ubicados en uno o varios centros de datos en las instalaciones o en la nube.</p>\n<p><strong>Clientes</strong> : los clientes de Kafka nos permiten escribir sistemas/aplicaciones de sistemas distribuidos que leen, escriben y procesan flujos de eventos con un enfoque tolerante a fallas en caso de falla de la red o de la máquina. Los clientes están disponibles como API REST y en varios lenguajes de programación, incluidos Java, Scala, Go, Python, C/C++ y muchos otros. En este tutorial nos centraremos en usar el cliente de python.</p>\n<p>Hay varios clientes que podemos usar para comunicarnos con Kafka</p>\n<ol>\n<li>Línea de comando</li>\n<li><a href=\"https://github.com/confluentinc/confluent-kafka-python\">Kafka confluente</a></li>\n<li>kafka-python (lo que estaríamos usando)</li>\n</ol>\n<p><strong>Instalación</strong> :</p>\n<p><strong>PASO 1:</strong></p>\n<p>Descarga Kafka desde <a href=\"https://www.apache.org/dyn/closer.cgi?path=/kafka/2.7.0/kafka_2.13-2.7.0.tgz\">aquí</a></p>\n<p>Comando</p>\n<p>Comando</p>\n<p><strong>PASO 2:</strong></p>\n<p><strong>NOTA</strong> : Su entorno local debe tener instalado Java 8+.</p>\n<p>Abra una terminal y ejecute este comando:</p>\n<p>Abra otra terminal y ejecute este comando:</p>\n<p><strong>PASO 3</strong> :</p>\n<p>Creación de un Tópico para almacenar eventos</p>\n<p>Ejecute este comando en otra terminal:</p>\n<p>Ejecute este comando para ver el Tópico:</p>\n<p>Que debería devolver algo como esto:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/800/1*OsmhHgVJkFmL0C4JQ1LA8A.png\" alt=\"\"></p>\n<h3 id=\"paso-4\">PASO 4:</h3>\n<p>Ejecute esto en su terminal para escribir un evento en un tópico</p>\n<h3 id=\"paso-5\">PASO 5:</h3>\n<p>Ejecute esto en su terminal para leer el evento del tópico</p>\n<p><strong>Zookeeper</strong> es un sistema de archivos consistente para la información de configuración que Kafka se usa para administrar y coordinar clústeres/intermediarios, lo que incluye la elección de leadership para la partición de tópicos de intermediarios.</p>\n<p><strong>Agente de Kafka</strong> : los clústeres de Kafka se componen de múltiples agentes, cada uno de los cuales tiene una identificación única. Cada agente que contiene particiones de registros de tópicos que conectan un cliente de arranque del agente con todo el cliente de Kafka.</p>\n<p>Con los pasos resaltados anteriormente, ahora tenemos una instancia de Kafka en ejecución en nuestra máquina. Antes de continuar, familiaricémonos con los conceptos de cómo funciona Kafka y los componentes que implica.</p>\n<p><strong>Conceptos de Kafka</strong></p>\n<p><img src=\"https://cdn-images-1.medium.com/max/800/0*DPsVvc5Cxgf2nQjO.png\" alt=\"\"></p>\n<p><strong>Eventos</strong> : significa que algo sucedió, es decir, se generan datos en una parte particular del sistema que nos interesa, por lo que se escribe un registro/mensaje en un tópico designado. Por lo tanto, un evento se registra en un formato de clave, valor y marca de tiempo para cada evento escrito.</p>\n<p><strong>Tópico</strong> : el tópico de Kafka se dividió en diferentes depósitos en varios centros de datos en todas las regiones para garantizar la tolerancia a fallas. También garantiza que los eventos se almacenen en el orden en que se escriben agregando nuevos eventos que llegan a los existentes y se replican en varias particiones en diferentes particiones. Nota Cada tópico se identifica mediante un nombre.</p>\n<p><strong>Producer</strong> : son aplicaciones de cliente escritas en cualquiera de los clientes de Kafka disponibles para escribir (publicar) eventos únicamente, es decir, mensajes/registros en su tópico designado, que se identifica con un nombre de tópico.<br>\nEstán escritos para ser agnósticos del consumidor, es decir, el productor no está al tanto de la aplicación del consumidor, hace un trabajo y lo hace bien al<br>\nescribir eventos sobre el tópico.</p>\n<p>Consumidores: son aplicaciones cliente para consumir eventos, es decir, mensajes/registros en el orden en que llegaron a un tema desde un tópico específico.</p>\n<p><strong>UTILIZANDO KAFKA-PYTHON</strong></p>\n<p>Para este tutorial, se supone que se está familiarizado con el lenguaje de programación python y los entornos virtuales python. Usaremos pipenv como nuestro entorno virtual para este tutorial. Y usaremos un cliente de kafka python de código abierto llamado <a href=\"https://kafka-python.readthedocs.io/\">kafka-python</a> github.</p>\n<p>Configuraríamos nuestro entorno virtual con pipenv ejecutando este comando <code>pipenv shell</code>e instalaríamos kafka-python con</p>\n<p><code>pip install kafka-python</code>.</p>\n<p>Antes de continuar, debemos analizar brevemente algunos términos clave al trabajar con <code>kafka-python</code> en el cliente.</p>\n<h3 id=\"kafkaproducer\"><code>KafkaProducer</code></h3>\n<p><code>**KafkaProducer**</code>es el cliente responsable de publicar registros en un clúster de Kafka. Lo hace llamando al método de <strong>envío</strong> que es asíncrono y cuando se llama agrega el registro a un búfer de registros pendientes, regresa inmediatamente. Además, el producer vuelve a intentarlo automáticamente si la solicitud falla, a menos que esté configurado de otra manera, que es una de las configuraciones que se pueden establecer.</p>\n<p>Vamos a crear un<code>KafkaProducer</code></p>\n<p><img src=\"https://cdn-images-1.medium.com/max/800/1*lGvPJAmtmIuoEIVbGFMcJA.png\" alt=\"\"></p>\n<p>Hagamos un recorrido rápido de lo que está sucediendo en el fragmento de código anterior.</p>\n<p><code>KafkaProducer</code>es la clase utilizada por <code>kafka-python</code>el cliente de python para instanciar una conexión al clúster de Kafka.</p>\n<p>bootstrap_servers es una lista de host[:port] con los que el productor debe ponerse en contacto para arrancar los metadatos del clúster inicial.</p>\n<p>Ahora enviamos el registro del producer llamando al método de envío que toma el argumento del nombre del tema, que es una cadena en este caso <strong>, el tópico del pedido</strong> , el mensaje, la clave, el valor, la marca de tiempo y algunos otros argumentos opcionales.</p>\n<p>Ahora, para el flujo síncrono, podrían ser errores, tal vez el nombre del tópico no se encontró . <code>kafka-python</code>El cliente lanza la <code>KafkaError</code>excepción que podemos manejar y tratar de manera adecuada.</p>\n<p>También podríamos enviar registros codificados mediante <code>msgpack</code>el cual producirá mensajes json. Así es como se vería:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/800/1*Ue0pdUZowRaBzlI8dbznyw.png\" alt=\"\"></p>\n<blockquote>\n<p>Nota: Hay más configuraciones que se pueden configurar en <code>KafakProducer</code>la <a href=\"https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html\">documentación</a> para ver más configuraciones que se pueden configurar.</p>\n</blockquote>\n<h3 id=\"kafkaconsumer\"><code>KafkaConsumer</code></h3>\n<p>Registros de suscripción (lectura) del consumidor del clúster de Kafka. El consumidor manejará de manera transparente la falla de los servidores en el clúster de Kafka y se adaptará a medida que se creen particiones de tópicos o migren entre intermediarios.</p>\n<p>Creemos Kafka Consumer:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/800/1*FFXBacF-MUkB20O7rppX8A.png\" alt=\"\"></p>\n<p>Veamos lo que está pasando en el fragmento de código del Consumidor</p>\n<p><code>KafkaConsumer</code></p>\n<p><code>bootstrap_servers</code>– Cadena ‘host[:port]’ (o lista de cadenas ‘host[:port]’) con la que el consumidor debe ponerse en contacto para iniciar los metadatos del clúster inicial.</p>\n<p><code>group_id</code>- Es el nombre del grupo de consumidores al que se puede unir dinámicamente si la asignación de partición está habilitada, que se usa para obtener y confirmar compensaciones.</p>\n<p><code>value_deserializer</code>(devolución de llamada) es cualquier invocable que toma un valor de mensaje sin procesar y devuelve un valor deserializado.</p>\n<p>Varios enfoques para consumir registros de un tópico</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/800/1*3V0K8mvHEqyPue7YjQ9y2g.png\" alt=\"\"></p>\n<p><strong>Conclusión</strong></p>\n<p>¡Uf!, si llegas hasta aquí, te doy las gracias. Solo hemos arañado la superficie de lo que podemos hacer con Kafka, hay muchas más cosas que se pueden lograr ampliando los argumentos en la <code>KafkaProducer</code>autenticación <code>KafkaConsumer</code>usando SSL, configurando el certificado SSL, agregando un nuevo tópico dinámicamente. Podemos explorar más configuraciones de la <a href=\"https://kafka-python.readthedocs.io/en/master/usage.html\">documentación</a> <code>kafka-python</code> .</p>\n<p>By <a href=\"https://medium.com/@devjaime\">Jaime Hernández</a> on <a href=\"https://medium.com/p/e34b727a472a\">April 16, 2022</a>.</p>\n<p><a href=\"https://medium.com/@devjaime/introducci%C3%B3n-a-c%C3%B3mo-funciona-kafka-e-implementaci%C3%B3n-usando-python-client-e34b727a472a\">Canonical link</a></p>\n<p>Exported from <a href=\"https://medium.com\">Medium</a> on March 15, 2025.</p>";

				const frontmatter = {"layout":"../../layouts/BlogLayout.astro","title":"Introducción a cómo funciona Kafka e implementación usando Python-client","description":"","tags":["code","Kafka","python"],"time":4,"featured":true,"timestamp":"2022-04-16T12:20:31-0300","filename":"2022-04-16_Introducci-n-a-c-mo-funciona-Kafka-e-implementaci-n-usando-Python-client-e34b727a472a"};
				const file = "/Users/devjaime/Documents/devjaimeblog/src/pages/blog/2022-04-16_Introducci-n-a-c-mo-funciona-Kafka-e-implementaci-n-usando-Python-client-e34b727a472a.md";
				const url = "/blog/2022-04-16_Introducci-n-a-c-mo-funciona-Kafka-e-implementaci-n-usando-Python-client-e34b727a472a";
				function rawContent() {
					return "   \n                                        \n                                                                                 \n               \n                                 \n       \n              \n                                     \n                                                                                                            \n   \n\n\nIntroducción a cómo funciona Kafka e implementación usando Python-client\n========================================================================\n\nConfigure un cliente de Python para Kafka con kafka-python\n\n* * *\n\n### Introducción a cómo funciona Kafka e implementación usando Python-client\n\n![](https://cdn-images-1.medium.com/max/800/1*BqSRfLZY2b4wZ6qg1fBTuw.png)\n\n* * *\n\n### Configure un cliente de Python para Kafka con kafka-python\n\nEl uso de datos en tiempo real se ha convertido en el uso comercial recurrente tanto para las empresas como para sus clientes. Sin embargo, uno de los factores clave a tener en cuenta es cómo el caso de uso comercial se basa en sus datos para el uso en tiempo real, es decir, ¿el caso de uso escribe más datos de los que lee, más de lectura que de escritura.  \nEs por esto que necesariamente se necesita procesar datos en tiempo real y en un enfoque basado en eventos, aquí es donde entra en juego Apache Kafka. Repasaremos qué es Kafka, los conceptos de Kafka, quién lo está usando, cómo configurarlo y cómo usarlo con un cliente Python ( `kafka-python`) en este tutorial.\n\n**¿Qué es Apache Kafka?**\n\nKafka es un sistema de mensajería distribuida de transmisión de eventos que consta de servidores y clientes que se comunican a través del protocolo de red TCP de alto rendimiento.  \n.\n\n> Nota: Kafka se desarrolló en Linkedin, pero ahora se administra bajo la fundación Apache, por lo tanto, Apache Kafka. Me referiré a Apache Kafka como Kafka a lo largo de este tutorial.\n\n**Transmisión de eventos**\n\nLa transmisión de eventos es la captura, el procesamiento y la transformación de datos en tiempo real en varios eventos de diferentes fuentes, por ejemplo, clicks en sitios web, bases de datos, sistemas de registro, dispositivos IOT, etc.\n\nal mismo tiempo que garantiza el flujo continuo y el enrutamiento de datos de flujo a varios destinos anticipando los datos del evento.\n\n**¿Por qué Kafka?**\n\nKafka se utiliza en arquitecturas de datos de transmisión de eventos en tiempo real para proporcionar análisis de datos, los mensajes se almacenan en el disco con Kafka, lo que proporciona replicación dentro del clúster, lo que hace que los mensajes sean más duraderos, más confiables y admitan múltiples suscriptores.\n\nKafka puede transmitir eventos continuamente mediante el uso  \ndel modelo de publicación-suscripción (pub-sub) en el que los eventos se pueden leer (suscribir).\n\nTan pronto como se escriben (publicar), procesar o incluso almacenar para la  \nretención de datos durante un período como Kafka da la flexibilidad sobre cuánto tiempo retener (almacenar) los datos.\n\n**¿Por qué Kafka es tan rápido?**\n\nKafka es rápido por varias razones, destacaremos algunas de estas razones a continuación.\n\n1.  **Copia cero**: se basa en gran medida en el principio [de copia cero,](https://en.wikipedia.org/wiki/Zero-copy) es decir, interactúa directamente con el kernel del sistema operativo para mover datos.\n2.  **Procesamiento por lotes**: permite el procesamiento por lotes de datos en fragmentos, lo que permite una compresión de datos eficiente y, por lo tanto, reduce la latencia de E/S.\n3.  **Escalado horizontal**: Kafka permite el escalado horizontal, ya que permite múltiples particiones (incluso en miles) sobre un tema que podría estar en miles de máquinas, ya sea en las instalaciones o en la nube, lo que lo hace muy capaz de soportar grandes cargas.\n4.  **Evitar la memoria RAM**: Kafka escribe en un registro de compromiso inmutable en el disco secuencialmente, evitando así la búsqueda lenta del disco.\n\n**¿Qué problema resuelve Kafka?**\n\nCon el auge de la innovación en varios aspectos de la vida desde Internet de  \nlas cosas (IOT), automóviles autónomos, inteligencia artificial, soluciones de cadena de bloques, robótica y muchos más, por mencionar algunos, la tasa de generación de datos está creciendo exponencialmente y no es ralentizando en el corto plazo.\n\nPor lo tanto, para que las empresas innoven y entiendan más a sus clientes y brinden mejores servicios, la forma tradicional de desarrollo de software debe mejorarse para incorporar el flujo de este enorme y creciente conjunto de datos de varias fuentes de datos, incluidas las  \nmencionadas y otras. Con Kafka, todos los diversos componentes del  \nsistema pueden comunicarse en un enfoque basado en eventos donde un evento de una parte del sistema se traduce en acción en otra parte del  \nsistema, la belleza de esto es que sucederá en tiempo real.\n\n**¿Qué empresas usan Kafka?**\n\nMiles de empresas están utilizando Kafka en la producción, incluidas las  \nempresas como Walmart o Uber, algunas de las empresas incluyen Microsoft, Netflix, Goldman, Sachs, Target, Cisco, Intuit, Box, Pinterest, New York Times y muchas [más](https://kafka.apache.org/powered-by) .\n\n**Primeros pasos con Kafka.**\n\nKafka implica la comunicación entre servidores y clientes.\n\n**Servidores** : Kafka se ejecuta como un clúster de uno o más servidores que pueden estar ubicados en uno o varios centros de datos en las instalaciones o en la nube.\n\n**Clientes** : los clientes de Kafka nos permiten escribir sistemas/aplicaciones de sistemas distribuidos que leen, escriben y procesan flujos de eventos con un enfoque tolerante a fallas en caso de falla de la red o de la máquina. Los clientes están disponibles como API REST y en varios lenguajes de programación, incluidos Java, Scala, Go, Python, C/C++ y muchos otros. En este tutorial nos centraremos en usar el cliente de python.\n\nHay varios clientes que podemos usar para comunicarnos con Kafka\n\n1.  Línea de comando\n2.  [Kafka confluente](https://github.com/confluentinc/confluent-kafka-python)\n3.  kafka-python (lo que estaríamos usando)\n\n**Instalación** :\n\n**PASO 1:**\n\nDescarga Kafka desde [aquí](https://www.apache.org/dyn/closer.cgi?path=/kafka/2.7.0/kafka_2.13-2.7.0.tgz)\n\nComando\n\nComando\n\n**PASO 2:**\n\n**NOTA** : Su entorno local debe tener instalado Java 8+.\n\nAbra una terminal y ejecute este comando:\n\nAbra otra terminal y ejecute este comando:\n\n**PASO 3** :\n\nCreación de un Tópico para almacenar eventos\n\nEjecute este comando en otra terminal:\n\nEjecute este comando para ver el Tópico:\n\nQue debería devolver algo como esto:\n\n![](https://cdn-images-1.medium.com/max/800/1*OsmhHgVJkFmL0C4JQ1LA8A.png)\n\n### PASO 4:\n\nEjecute esto en su terminal para escribir un evento en un tópico\n\n### PASO 5:\n\nEjecute esto en su terminal para leer el evento del tópico\n\n**Zookeeper** es un sistema de archivos consistente para la información de configuración que Kafka se usa para administrar y coordinar clústeres/intermediarios, lo que incluye la elección de leadership para la partición de tópicos de intermediarios.\n\n**Agente de Kafka** : los clústeres de Kafka se componen de múltiples agentes, cada uno de los cuales tiene una identificación única. Cada agente que contiene particiones de registros de tópicos que conectan un cliente de arranque del agente con todo el cliente de Kafka.\n\nCon los pasos resaltados anteriormente, ahora tenemos una instancia de Kafka en ejecución en nuestra máquina. Antes de continuar, familiaricémonos con los conceptos de cómo funciona Kafka y los componentes que implica.\n\n**Conceptos de Kafka**\n\n![](https://cdn-images-1.medium.com/max/800/0*DPsVvc5Cxgf2nQjO.png)\n\n**Eventos** : significa que algo sucedió, es decir, se generan datos en una parte particular del sistema que nos interesa, por lo que se escribe un registro/mensaje en un tópico designado. Por lo tanto, un evento se registra en un formato de clave, valor y marca de tiempo para cada evento escrito.\n\n**Tópico** : el tópico de Kafka se dividió en diferentes depósitos en varios centros de datos en todas las regiones para garantizar la tolerancia a fallas. También garantiza que los eventos se almacenen en el orden en que se escriben agregando nuevos eventos que llegan a los existentes y se replican en varias particiones en diferentes particiones. Nota Cada tópico se identifica mediante un nombre.\n\n**Producer** : son aplicaciones de cliente escritas en cualquiera de los clientes de Kafka disponibles para escribir (publicar) eventos únicamente, es decir, mensajes/registros en su tópico designado, que se identifica con un nombre de tópico.  \nEstán escritos para ser agnósticos del consumidor, es decir, el productor no está al tanto de la aplicación del consumidor, hace un trabajo y lo hace bien al  \nescribir eventos sobre el tópico.\n\nConsumidores: son aplicaciones cliente para consumir eventos, es decir, mensajes/registros en el orden en que llegaron a un tema desde un tópico específico.\n\n**UTILIZANDO KAFKA-PYTHON**\n\nPara este tutorial, se supone que se está familiarizado con el lenguaje de programación python y los entornos virtuales python. Usaremos pipenv como nuestro entorno virtual para este tutorial. Y usaremos un cliente de kafka python de código abierto llamado [kafka-python](https://kafka-python.readthedocs.io/) github.\n\nConfiguraríamos nuestro entorno virtual con pipenv ejecutando este comando `pipenv shell`e instalaríamos kafka-python con\n\n`pip install kafka-python`.\n\nAntes de continuar, debemos analizar brevemente algunos términos clave al trabajar con `kafka-python` en el cliente.\n\n### `KafkaProducer`\n\n`**KafkaProducer**`es el cliente responsable de publicar registros en un clúster de Kafka. Lo hace llamando al método de **envío** que es asíncrono y cuando se llama agrega el registro a un búfer de registros pendientes, regresa inmediatamente. Además, el producer vuelve a intentarlo automáticamente si la solicitud falla, a menos que esté configurado de otra manera, que es una de las configuraciones que se pueden establecer.\n\nVamos a crear un`KafkaProducer`\n\n![](https://cdn-images-1.medium.com/max/800/1*lGvPJAmtmIuoEIVbGFMcJA.png)\n\nHagamos un recorrido rápido de lo que está sucediendo en el fragmento de código anterior.\n\n`KafkaProducer`es la clase utilizada por `kafka-python`el cliente de python para instanciar una conexión al clúster de Kafka.\n\nbootstrap\\_servers es una lista de host\\[:port\\] con los que el productor debe ponerse en contacto para arrancar los metadatos del clúster inicial.\n\nAhora enviamos el registro del producer llamando al método de envío que toma el argumento del nombre del tema, que es una cadena en este caso **, el tópico del pedido** , el mensaje, la clave, el valor, la marca de tiempo y algunos otros argumentos opcionales.\n\nAhora, para el flujo síncrono, podrían ser errores, tal vez el nombre del tópico no se encontró . `kafka-python`El cliente lanza la `KafkaError`excepción que podemos manejar y tratar de manera adecuada.\n\nTambién podríamos enviar registros codificados mediante `msgpack`el cual producirá mensajes json. Así es como se vería:\n\n![](https://cdn-images-1.medium.com/max/800/1*Ue0pdUZowRaBzlI8dbznyw.png)\n\n> Nota: Hay más configuraciones que se pueden configurar en `KafakProducer`la [documentación](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html) para ver más configuraciones que se pueden configurar.\n\n### `KafkaConsumer`\n\nRegistros de suscripción (lectura) del consumidor del clúster de Kafka. El consumidor manejará de manera transparente la falla de los servidores en el clúster de Kafka y se adaptará a medida que se creen particiones de tópicos o migren entre intermediarios.\n\nCreemos Kafka Consumer:\n\n![](https://cdn-images-1.medium.com/max/800/1*FFXBacF-MUkB20O7rppX8A.png)\n\nVeamos lo que está pasando en el fragmento de código del Consumidor\n\n`KafkaConsumer`\n\n`bootstrap_servers`– Cadena 'host\\[:port\\]' (o lista de cadenas 'host\\[:port\\]') con la que el consumidor debe ponerse en contacto para iniciar los metadatos del clúster inicial.\n\n`group_id`\\- Es el nombre del grupo de consumidores al que se puede unir dinámicamente si la asignación de partición está habilitada, que se usa para obtener y confirmar compensaciones.\n\n`value_deserializer`(devolución de llamada) es cualquier invocable que toma un valor de mensaje sin procesar y devuelve un valor deserializado.\n\nVarios enfoques para consumir registros de un tópico\n\n![](https://cdn-images-1.medium.com/max/800/1*3V0K8mvHEqyPue7YjQ9y2g.png)\n\n**Conclusión**\n\n¡Uf!, si llegas hasta aquí, te doy las gracias. Solo hemos arañado la superficie de lo que podemos hacer con Kafka, hay muchas más cosas que se pueden lograr ampliando los argumentos en la `KafkaProducer`autenticación `KafkaConsumer`usando SSL, configurando el certificado SSL, agregando un nuevo tópico dinámicamente. Podemos explorar más configuraciones de la [documentación](https://kafka-python.readthedocs.io/en/master/usage.html) `kafka-python` .\n\nBy [Jaime Hernández](https://medium.com/@devjaime) on [April 16, 2022](https://medium.com/p/e34b727a472a).\n\n[Canonical link](https://medium.com/@devjaime/introducci%C3%B3n-a-c%C3%B3mo-funciona-kafka-e-implementaci%C3%B3n-usando-python-client-e34b727a472a)\n\nExported from [Medium](https://medium.com) on March 15, 2025.";
				}
				async function compiledContent() {
					return await html();
				}
				function getHeadings() {
					return [{"depth":1,"slug":"introducción-a-cómo-funciona-kafka-e-implementación-usando-python-client","text":"Introducción a cómo funciona Kafka e implementación usando Python-client"},{"depth":3,"slug":"introducción-a-cómo-funciona-kafka-e-implementación-usando-python-client-1","text":"Introducción a cómo funciona Kafka e implementación usando Python-client"},{"depth":3,"slug":"configure-un-cliente-de-python-para-kafka-con-kafka-python","text":"Configure un cliente de Python para Kafka con kafka-python"},{"depth":3,"slug":"paso-4","text":"PASO 4:"},{"depth":3,"slug":"paso-5","text":"PASO 5:"},{"depth":3,"slug":"kafkaproducer","text":"KafkaProducer"},{"depth":3,"slug":"kafkaconsumer","text":"KafkaConsumer"}];
				}

				const Content = createComponent((result, _props, slots) => {
					const { layout, ...content } = frontmatter;
					content.file = file;
					content.url = url;

					return renderTemplate`${renderComponent(result, 'Layout', $$BlogLayout, {
								file,
								url,
								content,
								frontmatter: content,
								headings: getHeadings(),
								rawContent,
								compiledContent,
								'server:root': true,
							}, {
								'default': () => renderTemplate`${unescapeHTML(html())}`
							})}`;
				});

const _page = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
	__proto__: null,
	Content,
	compiledContent,
	default: Content,
	file,
	frontmatter,
	getHeadings,
	rawContent,
	url
}, Symbol.toStringTag, { value: 'Module' }));

export { _page as _ };
