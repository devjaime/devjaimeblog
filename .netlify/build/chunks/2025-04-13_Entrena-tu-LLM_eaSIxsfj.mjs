/* empty css                                                                    */
import { d as createComponent, i as renderComponent, r as renderTemplate, u as unescapeHTML } from './astro/server_C7nAViGe.mjs';
import 'kleur/colors';
import { $ as $$BlogLayout } from './BlogLayout_COI89YL8.mjs';

const html = () => "<h2 id=\"-introducción\">🌟 Introducción</h2>\n<p>Entrenar tu propio modelo de lenguaje (LLM) ya no es exclusivo de grandes laboratorios de investigación. Gracias al trabajo de Imad Saddik, y el excelente artículo de <a href=\"https://www.freecodecamp.org/news/train-your-own-llm/\">freeCodeCamp</a>, hoy puedes aprender paso a paso cómo construir, entrenar y ajustar tu propio modelo.</p>\n<p>Este artículo es una guía detallada basada en:</p>\n<ul>\n<li>El repositorio <a href=\"https://github.com/ImadSaddik/Train_Your_Language_Model_Course\">Train_Your_Language_Model_Course</a></li>\n<li>El video de YouTube <a href=\"https://www.youtube.com/watch?v=9Ge0sMm65jo&#x26;t=2718s\">Train Your Own LLM – Tutorial</a></li>\n<li>La explicación técnica de freeCodeCamp</li>\n</ul>\n<p>Además, te explicamos <strong>por qué cada concepto es importante, cómo funciona el aprendizaje de los modelos, por qué requieren GPU y cómo especializarlos según casos de uso reales.</strong></p>\n<hr>\n<h2 id=\"-qué-es-una-red-neuronal\">🧠 ¿Qué es una red neuronal?</h2>\n<p>Una red neuronal es una estructura computacional inspirada en el cerebro humano. Está formada por capas de <strong>neuronas artificiales</strong> que procesan datos numéricos. Estas redes aprenden a partir de ejemplos, ajustando sus <strong>pesos y sesgos</strong> internos para minimizar el error en sus predicciones.</p>\n<p>Una red neuronal se compone de:</p>\n<ul>\n<li><strong>Capa de entrada</strong>: donde ingresan los datos (por ejemplo, los tokens del texto).</li>\n<li><strong>Capas ocultas</strong>: donde ocurre el procesamiento y transformación del conocimiento.</li>\n<li><strong>Capa de salida</strong>: que entrega el resultado final (por ejemplo, la palabra siguiente que predice un LLM).</li>\n</ul>\n<p>Cada conexión entre neuronas tiene un <strong>peso</strong> ajustable que determina la importancia de la señal. Durante el entrenamiento, se ajustan esos pesos para mejorar la salida.</p>\n<p>🔄 <strong>¿Por qué es clave?</strong>\nPorque todos los modelos de lenguaje (LLMs) son en el fondo redes neuronales profundas. Entender esto te ayuda a comprender su capacidad de aprendizaje, generalización y ajuste.</p>\n<hr>\n<h2 id=\"-cómo-aprenden-los-modelos-llm\">🧬 ¿Cómo aprenden los modelos LLM?</h2>\n<p>Los modelos de lenguaje funcionan como grandes redes neuronales entrenadas para predecir la siguiente palabra en una secuencia de texto. Su aprendizaje ocurre durante el entrenamiento:</p>\n<ul>\n<li>El modelo ve un texto, por ejemplo: “La capital de Francia es”</li>\n<li>Predice la siguiente palabra: “París”</li>\n<li>Compara su predicción con la correcta y ajusta sus pesos internos</li>\n</ul>\n<p>Este proceso se repite millones o incluso billones de veces, mejorando la comprensión del contexto, sintaxis y significado.</p>\n<p>⚙️ <strong>Esto se logra gracias al mecanismo de atención</strong> de los Transformers, que permite al modelo enfocarse en partes relevantes del texto para entender relaciones semánticas.</p>\n<hr>\n<h2 id=\"-por-qué-se-necesita-una-gpu-para-entrenar-llms\">🚀 ¿Por qué se necesita una GPU para entrenar LLMs?</h2>\n<ul>\n<li>Los modelos tienen millones o billones de parámetros (pesos). Procesar esos datos en CPU es extremadamente lento.</li>\n<li>Las GPUs permiten computación paralela en cientos o miles de núcleos.</li>\n<li>Entrenar un modelo como GPT-2 pequeño puede tardar días en CPU, pero solo horas en una buena GPU.</li>\n</ul>\n<p>🔌 <strong>¿No tienes GPU?</strong> Puedes usar Google Colab, Paperspace o servicios cloud como AWS o GCP con GPU gratis o por horas.</p>\n<hr>\n<h2 id=\"-casos-de-uso-y-especializaciones-posibles\">🧭 Casos de uso y especializaciones posibles</h2>\n<p>Entrenar tu propio LLM no solo es útil para aprender: puede ser una ventaja competitiva real. Aquí algunos ejemplos:</p>\n<ul>\n<li>🤖 <strong>Asistentes virtuales personalizados</strong> para sectores como salud, legal, educación, retail.</li>\n<li>📄 <strong>Indexación y comprensión de documentos internos</strong>, por ejemplo, políticas, contratos o manuales.</li>\n<li>🌎 <strong>Modelos en otros idiomas</strong>: entrenar un modelo en quechua, mapudungún o jergas técnicas locales.</li>\n<li>📝 <strong>Generadores de contenido específicos</strong> para newsletters, posts técnicos, o atención al cliente.</li>\n<li>🧠 <strong>Modelos educativos</strong> que se adaptan a la forma de hablar del estudiante.</li>\n</ul>\n<p>🧩 Puedes extender tu modelo:</p>\n<ul>\n<li>Entrenando con datos adicionales (fine-tuning)</li>\n<li>Combinándolo con bases vectoriales para RAG</li>\n<li>Integrándolo a interfaces como chatbots, APIs, LangChain o Langflow</li>\n</ul>\n<h2 id=\"-pasos-técnicos-para-entrenar-tu-propio-llm-detallado\">🧪 Pasos técnicos para entrenar tu propio LLM (detallado)</h2>\n<p>Aquí detallamos cada paso técnico para que puedas seguir el proceso completo desde el código hasta la práctica.</p>\n<hr>\n<h3 id=\"-paso-1-preparar-el-entorno\">📁 Paso 1: Preparar el entorno</h3>\n<p>Instala Python y las dependencias necesarias:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\"><code><span class=\"line\"><span style=\"color:#B392F0\">pip</span><span style=\"color:#9ECBFF\"> install</span><span style=\"color:#9ECBFF\"> datasets</span><span style=\"color:#9ECBFF\"> transformers</span><span style=\"color:#9ECBFF\"> accelerate</span><span style=\"color:#9ECBFF\"> tokenizers</span><span style=\"color:#9ECBFF\"> torch</span></span></code></pre>\n<p>Crea tu entorno virtual:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> venv</span><span style=\"color:#9ECBFF\"> llm-env</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">source</span><span style=\"color:#9ECBFF\"> llm-env/bin/activate</span><span style=\"color:#6A737D\">  # En Windows: llm-env\\Scripts\\activate</span></span></code></pre>\n<hr>\n<h3 id=\"-paso-2-dataset-personalizado\">🌐 Paso 2: Dataset personalizado</h3>\n<p>Carga un dataset propio o alguno publicado en Hugging Face:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datasets </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> load_dataset</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Puedes usar uno propio o alguno público como el de Imad Saddik</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> load_dataset(</span><span style=\"color:#9ECBFF\">\"ImadSaddik/BoDmaghDataset\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre>\n<hr>\n<h3 id=\"-paso-3-tokenización\">🔎 Paso 3: Tokenización</h3>\n<p>Convierte texto en tokens que el modelo pueda procesar:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AutoTokenizer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model_ckpt </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"gpt2\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AutoTokenizer.from_pretrained(model_ckpt)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> tokenize</span><span style=\"color:#E1E4E8\">(example):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tokenizer(example[</span><span style=\"color:#9ECBFF\">\"text\"</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">truncation</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokenized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data.map(tokenize, </span><span style=\"color:#FFAB70\">batched</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span></code></pre>\n<hr>\n<h3 id=\"-paso-4-configurar-el-modelo-y-entrenamiento\">🧠 Paso 4: Configurar el modelo y entrenamiento</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AutoModelForCausalLM, Trainer, TrainingArguments</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AutoModelForCausalLM.from_pretrained(model_ckpt)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">args </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TrainingArguments(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    output_dir</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"output\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    evaluation_strategy</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"epoch\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    learning_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2e-5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    per_device_train_batch_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    num_train_epochs</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">trainer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Trainer(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    model</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">model,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    args</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">args,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    train_dataset</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokenized[</span><span style=\"color:#9ECBFF\">\"train\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    eval_dataset</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokenized[</span><span style=\"color:#9ECBFF\">\"test\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">trainer.train()</span></span></code></pre>\n<hr>\n<h3 id=\"-paso-5-guardar-el-modelo-entrenado\">💾 Paso 5: Guardar el modelo entrenado</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">model.save_pretrained(</span><span style=\"color:#9ECBFF\">\"./my-llm\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokenizer.save_pretrained(</span><span style=\"color:#9ECBFF\">\"./my-llm\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre>\n<hr>\n<h3 id=\"️-paso-6-usar-el-modelo-para-generar-texto\">✍️ Paso 6: Usar el modelo para generar texto</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pipeline</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">pipe </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pipeline(</span><span style=\"color:#9ECBFF\">\"text-generation\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">model</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"./my-llm\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tokenizer</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"./my-llm\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(pipe(</span><span style=\"color:#9ECBFF\">\"Hola, hoy vamos a\"</span><span style=\"color:#E1E4E8\">))</span></span></code></pre>\n<hr>\n<h3 id=\"-paso-7-personalizaciones-posibles\">🔧 Paso 7: Personalizaciones posibles</h3>\n<ul>\n<li>Usar modelos más pequeños como <code>distilgpt2</code> para computadoras sin GPU</li>\n<li>Ejecutar en Google Colab con soporte CUDA</li>\n<li>Añadir tus propios datos para fine-tuning específico</li>\n<li>Usar <code>accelerate</code> para mejorar la velocidad de entrenamiento</li>\n</ul>\n<hr>\n<p>Con estos pasos técnicos puedes construir, entrenar y usar un modelo de lenguaje personalizado. Este conocimiento te abre la puerta a crear soluciones innovadoras, adaptadas a tu idioma, sector o problema particular. 🚀</p>\n<h2 id=\"-reflexiones-finales-actualizado\">🧠 Reflexiones finales (actualizado)</h2>\n<p>Este enfoque te permite:</p>\n<ul>\n<li>Crear asistentes especializados</li>\n<li>Entrenar en tu propio idioma o jerga profesional</li>\n<li>Aprender en profundidad el funcionamiento interno de los LLMs</li>\n<li>Identificar nuevos productos y servicios basados en IA en tu startup o trabajo</li>\n</ul>\n<p>📈 Ya sea que trabajes en una startup, como freelance o en una empresa tech, entender estos pasos te posiciona como un experto capaz de construir soluciones basadas en IA de principio a fin.</p>\n<p>La práctica guiada por Imad Saddik y la comunidad de HuggingFace abre una nueva era para developers y emprendedores que quieran integrar inteligencia artificial en sus soluciones. ✨</p>\n<hr>\n<p>Si te interesa aplicar esto en tu startup, crear un chatbot propio o entrenar en datos privados, contáctame y lo armamos juntos. ✍️</p>";

				const frontmatter = {"layout":"../../layouts/BlogLayout.astro","title":"Entrena tu propio LLM paso a paso: conceptos, código y práctica","description":"Aprende cómo entrenar tu propio modelo de lenguaje (LLM) desde cero, comprendiendo cada componente: tokenización, entrenamiento, datasets, arquitectura, y más. Basado en el tutorial de freeCodeCamp e Imad Saddik.","tags":["LLM","Machine Learning","NLP","Python","Fine-Tuning","Entrenamiento","Tokenización","Transformers","IA"],"time":12,"timestamp":"2025-04-13T10:00:00-0300","featured":true,"filename":"2025-04-13_Entrena-tu-LLM"};
				const file = "/Users/devjaime/Documents/devjaimeblog/src/pages/blog/2025-04-13_Entrena-tu-LLM.md";
				const url = "/blog/2025-04-13_Entrena-tu-LLM";
				function rawContent() {
					return "   \n                                        \n                                                                        \n                                                                                                                                                                                                                                   \n                                                                                                                        \n        \n                                     \n              \n                                     \n   \n\n## 🌟 Introducción\n\nEntrenar tu propio modelo de lenguaje (LLM) ya no es exclusivo de grandes laboratorios de investigación. Gracias al trabajo de Imad Saddik, y el excelente artículo de [freeCodeCamp](https://www.freecodecamp.org/news/train-your-own-llm/), hoy puedes aprender paso a paso cómo construir, entrenar y ajustar tu propio modelo.\n\nEste artículo es una guía detallada basada en:\n- El repositorio [Train_Your_Language_Model_Course](https://github.com/ImadSaddik/Train_Your_Language_Model_Course)\n- El video de YouTube [Train Your Own LLM – Tutorial](https://www.youtube.com/watch?v=9Ge0sMm65jo&t=2718s)\n- La explicación técnica de freeCodeCamp\n\nAdemás, te explicamos **por qué cada concepto es importante, cómo funciona el aprendizaje de los modelos, por qué requieren GPU y cómo especializarlos según casos de uso reales.**\n\n---\n\n## 🧠 ¿Qué es una red neuronal?\n\nUna red neuronal es una estructura computacional inspirada en el cerebro humano. Está formada por capas de **neuronas artificiales** que procesan datos numéricos. Estas redes aprenden a partir de ejemplos, ajustando sus **pesos y sesgos** internos para minimizar el error en sus predicciones.\n\nUna red neuronal se compone de:\n- **Capa de entrada**: donde ingresan los datos (por ejemplo, los tokens del texto).\n- **Capas ocultas**: donde ocurre el procesamiento y transformación del conocimiento.\n- **Capa de salida**: que entrega el resultado final (por ejemplo, la palabra siguiente que predice un LLM).\n\nCada conexión entre neuronas tiene un **peso** ajustable que determina la importancia de la señal. Durante el entrenamiento, se ajustan esos pesos para mejorar la salida.\n\n🔄 **¿Por qué es clave?**\nPorque todos los modelos de lenguaje (LLMs) son en el fondo redes neuronales profundas. Entender esto te ayuda a comprender su capacidad de aprendizaje, generalización y ajuste.\n\n---\n\n## 🧬 ¿Cómo aprenden los modelos LLM?\n\nLos modelos de lenguaje funcionan como grandes redes neuronales entrenadas para predecir la siguiente palabra en una secuencia de texto. Su aprendizaje ocurre durante el entrenamiento:\n\n- El modelo ve un texto, por ejemplo: \"La capital de Francia es\"\n- Predice la siguiente palabra: \"París\"\n- Compara su predicción con la correcta y ajusta sus pesos internos\n\nEste proceso se repite millones o incluso billones de veces, mejorando la comprensión del contexto, sintaxis y significado.\n\n⚙️ **Esto se logra gracias al mecanismo de atención** de los Transformers, que permite al modelo enfocarse en partes relevantes del texto para entender relaciones semánticas.\n\n---\n\n## 🚀 ¿Por qué se necesita una GPU para entrenar LLMs?\n\n- Los modelos tienen millones o billones de parámetros (pesos). Procesar esos datos en CPU es extremadamente lento.\n- Las GPUs permiten computación paralela en cientos o miles de núcleos.\n- Entrenar un modelo como GPT-2 pequeño puede tardar días en CPU, pero solo horas en una buena GPU.\n\n🔌 **¿No tienes GPU?** Puedes usar Google Colab, Paperspace o servicios cloud como AWS o GCP con GPU gratis o por horas.\n\n---\n\n## 🧭 Casos de uso y especializaciones posibles\n\nEntrenar tu propio LLM no solo es útil para aprender: puede ser una ventaja competitiva real. Aquí algunos ejemplos:\n\n- 🤖 **Asistentes virtuales personalizados** para sectores como salud, legal, educación, retail.\n- 📄 **Indexación y comprensión de documentos internos**, por ejemplo, políticas, contratos o manuales.\n- 🌎 **Modelos en otros idiomas**: entrenar un modelo en quechua, mapudungún o jergas técnicas locales.\n- 📝 **Generadores de contenido específicos** para newsletters, posts técnicos, o atención al cliente.\n- 🧠 **Modelos educativos** que se adaptan a la forma de hablar del estudiante.\n\n🧩 Puedes extender tu modelo:\n- Entrenando con datos adicionales (fine-tuning)\n- Combinándolo con bases vectoriales para RAG\n- Integrándolo a interfaces como chatbots, APIs, LangChain o Langflow\n\n## 🧪 Pasos técnicos para entrenar tu propio LLM (detallado)\n\nAquí detallamos cada paso técnico para que puedas seguir el proceso completo desde el código hasta la práctica.\n\n---\n\n### 📁 Paso 1: Preparar el entorno\n\nInstala Python y las dependencias necesarias:\n\n```bash\npip install datasets transformers accelerate tokenizers torch\n```\n\nCrea tu entorno virtual:\n```bash\npython -m venv llm-env\nsource llm-env/bin/activate  # En Windows: llm-env\\Scripts\\activate\n```\n\n---\n\n### 🌐 Paso 2: Dataset personalizado\n\nCarga un dataset propio o alguno publicado en Hugging Face:\n\n```python\nfrom datasets import load_dataset\n\n# Puedes usar uno propio o alguno público como el de Imad Saddik\ndata = load_dataset(\"ImadSaddik/BoDmaghDataset\")\n```\n\n---\n\n### 🔎 Paso 3: Tokenización\n\nConvierte texto en tokens que el modelo pueda procesar:\n\n```python\nfrom transformers import AutoTokenizer\n\nmodel_ckpt = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\ndef tokenize(example):\n    return tokenizer(example[\"text\"], truncation=True)\n\ntokenized = data.map(tokenize, batched=True)\n```\n\n---\n\n### 🧠 Paso 4: Configurar el modelo y entrenamiento\n\n```python\nfrom transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n\nmodel = AutoModelForCausalLM.from_pretrained(model_ckpt)\n\nargs = TrainingArguments(\n    output_dir=\"output\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    num_train_epochs=3\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"test\"]\n)\n\ntrainer.train()\n```\n\n---\n\n### 💾 Paso 5: Guardar el modelo entrenado\n\n```python\nmodel.save_pretrained(\"./my-llm\")\ntokenizer.save_pretrained(\"./my-llm\")\n```\n\n---\n\n### ✍️ Paso 6: Usar el modelo para generar texto\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"./my-llm\", tokenizer=\"./my-llm\")\nprint(pipe(\"Hola, hoy vamos a\"))\n```\n\n---\n\n### 🔧 Paso 7: Personalizaciones posibles\n\n- Usar modelos más pequeños como `distilgpt2` para computadoras sin GPU\n- Ejecutar en Google Colab con soporte CUDA\n- Añadir tus propios datos para fine-tuning específico\n- Usar `accelerate` para mejorar la velocidad de entrenamiento\n\n---\n\nCon estos pasos técnicos puedes construir, entrenar y usar un modelo de lenguaje personalizado. Este conocimiento te abre la puerta a crear soluciones innovadoras, adaptadas a tu idioma, sector o problema particular. 🚀\n\n\n\n## 🧠 Reflexiones finales (actualizado)\n\nEste enfoque te permite:\n- Crear asistentes especializados\n- Entrenar en tu propio idioma o jerga profesional\n- Aprender en profundidad el funcionamiento interno de los LLMs\n- Identificar nuevos productos y servicios basados en IA en tu startup o trabajo\n\n📈 Ya sea que trabajes en una startup, como freelance o en una empresa tech, entender estos pasos te posiciona como un experto capaz de construir soluciones basadas en IA de principio a fin.\n\nLa práctica guiada por Imad Saddik y la comunidad de HuggingFace abre una nueva era para developers y emprendedores que quieran integrar inteligencia artificial en sus soluciones. ✨\n\n---\n\nSi te interesa aplicar esto en tu startup, crear un chatbot propio o entrenar en datos privados, contáctame y lo armamos juntos. ✍️\n\n";
				}
				async function compiledContent() {
					return await html();
				}
				function getHeadings() {
					return [{"depth":2,"slug":"-introducción","text":"🌟 Introducción"},{"depth":2,"slug":"-qué-es-una-red-neuronal","text":"🧠 ¿Qué es una red neuronal?"},{"depth":2,"slug":"-cómo-aprenden-los-modelos-llm","text":"🧬 ¿Cómo aprenden los modelos LLM?"},{"depth":2,"slug":"-por-qué-se-necesita-una-gpu-para-entrenar-llms","text":"🚀 ¿Por qué se necesita una GPU para entrenar LLMs?"},{"depth":2,"slug":"-casos-de-uso-y-especializaciones-posibles","text":"🧭 Casos de uso y especializaciones posibles"},{"depth":2,"slug":"-pasos-técnicos-para-entrenar-tu-propio-llm-detallado","text":"🧪 Pasos técnicos para entrenar tu propio LLM (detallado)"},{"depth":3,"slug":"-paso-1-preparar-el-entorno","text":"📁 Paso 1: Preparar el entorno"},{"depth":3,"slug":"-paso-2-dataset-personalizado","text":"🌐 Paso 2: Dataset personalizado"},{"depth":3,"slug":"-paso-3-tokenización","text":"🔎 Paso 3: Tokenización"},{"depth":3,"slug":"-paso-4-configurar-el-modelo-y-entrenamiento","text":"🧠 Paso 4: Configurar el modelo y entrenamiento"},{"depth":3,"slug":"-paso-5-guardar-el-modelo-entrenado","text":"💾 Paso 5: Guardar el modelo entrenado"},{"depth":3,"slug":"️-paso-6-usar-el-modelo-para-generar-texto","text":"✍️ Paso 6: Usar el modelo para generar texto"},{"depth":3,"slug":"-paso-7-personalizaciones-posibles","text":"🔧 Paso 7: Personalizaciones posibles"},{"depth":2,"slug":"-reflexiones-finales-actualizado","text":"🧠 Reflexiones finales (actualizado)"}];
				}

				const Content = createComponent((result, _props, slots) => {
					const { layout, ...content } = frontmatter;
					content.file = file;
					content.url = url;

					return renderTemplate`${renderComponent(result, 'Layout', $$BlogLayout, {
								file,
								url,
								content,
								frontmatter: content,
								headings: getHeadings(),
								rawContent,
								compiledContent,
								'server:root': true,
							}, {
								'default': () => renderTemplate`${unescapeHTML(html())}`
							})}`;
				});

const _page = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
	__proto__: null,
	Content,
	compiledContent,
	default: Content,
	file,
	frontmatter,
	getHeadings,
	rawContent,
	url
}, Symbol.toStringTag, { value: 'Module' }));

export { _page as _ };
